# -*- coding: utf-8 -*-
"""
Stage3æ¨¡å—æµ‹è¯•è„šæœ¬
æµ‹è¯•Stage3çš„é…ç½®åŠ è½½ã€å‰å‘ä¼ æ’­å’Œè·¨åˆ†è¾¨ç‡èåˆ
"""
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

import torch
from src.models.stage3 import create_stage3, CrossResolutionFusion3
from src.models.config_loader import get_stage3_config


def test_stage3():
    """æµ‹è¯•Stage3æ¨¡å—"""
    print("=" * 60)
    print("æµ‹è¯•Stage3æ¨¡å— - ä¸‰åˆ†æ”¯ç»“æ„")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    stage3_config = get_stage3_config()
    print("\né…ç½®ä¿¡æ¯:")
    print(f"  åˆ†æ”¯é€šé“: {stage3_config['channels']}")
    print(f"  å—æ•°é‡: {stage3_config['num_blocks']}")
    print(f"  æ‰©å¼ å€ç‡: {stage3_config['expansion']}")
    
    # åˆ›å»ºæ¨¡å—
    stage3 = create_stage3(stage3_config)
    stage3.eval()
    
    # å‡†å¤‡æµ‹è¯•è¾“å…¥ (æ¥è‡ªStage2çš„è¾“å‡º)
    batch_size = 2
    channels_1_4 = stage3_config['channels'][0]  # 32
    channels_1_8 = stage3_config['channels'][1]  # 64
    
    # Stage2çš„è¾“å‡ºä½œä¸ºStage3çš„è¾“å…¥
    y1 = torch.randn(batch_size, channels_1_4, 64, 128)  # 1/4åˆ†è¾¨ç‡
    y2 = torch.randn(batch_size, channels_1_8, 32, 64)   # 1/8åˆ†è¾¨ç‡
    inputs = [y1, y2]
    
    print(f"\nè¾“å…¥å¼ é‡:")
    print(f"  Y1 (1/4åˆ†è¾¨ç‡): {y1.shape}")
    print(f"  Y2 (1/8åˆ†è¾¨ç‡): {y2.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = stage3(inputs)
    
    # è¾“å‡ºä¿¡æ¯
    print(f"\nè¾“å‡ºä¿¡æ¯:")
    print(f"  è¾“å‡ºæ•°é‡: {len(outputs)}")
    print(f"  T1 (1/4åˆ†è¾¨ç‡): {outputs[0].shape}")
    print(f"  T2 (1/8åˆ†è¾¨ç‡): {outputs[1].shape}")
    print(f"  T3 (1/16åˆ†è¾¨ç‡): {outputs[2].shape}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_shapes = [
        (batch_size, stage3_config['channels'][0], 64, 128),   # 1/4
        (batch_size, stage3_config['channels'][1], 32, 64),    # 1/8
        (batch_size, stage3_config['channels'][2], 16, 32),    # 1/16
    ]
    
    for i, (output, expected) in enumerate(zip(outputs, expected_shapes)):
        assert output.shape == expected, f"è¾“å‡º{i}å½¢çŠ¶é”™è¯¯: {output.shape} != {expected}"
    
    print("\nâœ“ è¾“å‡ºå½¢çŠ¶éªŒè¯é€šè¿‡")
    
    # å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in stage3.parameters())
    trainable_params = sum(p.numel() for p in stage3.parameters() if p.requires_grad)
    
    print(f"\nå‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)")
    
    # åˆ†æ¨¡å—å‚æ•°ç»Ÿè®¡
    print(f"\nåˆ†æ¨¡å—å‚æ•°ç»Ÿè®¡:")
    
    # Transitionå‚æ•°
    t3_params = sum(p.numel() for name, p in stage3.named_parameters() if 't3_' in name)
    print(f"  Transition3æ¨¡å—: {t3_params:,}")
    
    # åˆ†æ”¯å‚æ•°
    branch_1_4_params = sum(p.numel() for name, p in stage3.named_parameters() if 'branch_1_4' in name)
    branch_1_8_params = sum(p.numel() for name, p in stage3.named_parameters() if 'branch_1_8' in name)
    branch_1_16_params = sum(p.numel() for name, p in stage3.named_parameters() if 'branch_1_16' in name)
    print(f"  1/4åˆ†æ”¯ (3Ã—LiteBlock): {branch_1_4_params:,}")
    print(f"  1/8åˆ†æ”¯ (3Ã—LiteBlock): {branch_1_8_params:,}")
    print(f"  1/16åˆ†æ”¯ (3Ã—LiteBlock): {branch_1_16_params:,}")
    
    # èåˆæ¨¡å—å‚æ•°
    fusion_params = sum(p.numel() for name, p in stage3.named_parameters() if 'fusion' in name)
    print(f"  CRF-3èåˆæ¨¡å—: {fusion_params:,}")
    
    print("\nâœ“ Stage3æ¨¡å—æµ‹è¯•å®Œæˆ")
    print("=" * 60)


def test_cross_resolution_fusion3():
    """æµ‹è¯•ä¸‰å‘è·¨åˆ†è¾¨ç‡èåˆæ¨¡å—"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•è·¨åˆ†è¾¨ç‡èåˆæ¨¡å— CRF-3")
    print("=" * 60)
    
    # åˆ›å»ºèåˆæ¨¡å—
    channels = [32, 64, 128]
    fusion = CrossResolutionFusion3(
        channels_1_4=channels[0],
        channels_1_8=channels[1],
        channels_1_16=channels[2]
    )
    fusion.eval()
    
    # æµ‹è¯•è¾“å…¥
    batch_size = 2
    x_1_4 = torch.randn(batch_size, channels[0], 64, 128)
    x_1_8 = torch.randn(batch_size, channels[1], 32, 64)
    x_1_16 = torch.randn(batch_size, channels[2], 16, 32)
    
    print(f"\nè¾“å…¥:")
    print(f"  1/4åˆ†è¾¨ç‡: {x_1_4.shape}")
    print(f"  1/8åˆ†è¾¨ç‡: {x_1_8.shape}")
    print(f"  1/16åˆ†è¾¨ç‡: {x_1_16.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        t1, t2, t3 = fusion(x_1_4, x_1_8, x_1_16)
    
    print(f"\nè¾“å‡º:")
    print(f"  T1 (1/4åˆ†è¾¨ç‡): {t1.shape}")
    print(f"  T2 (1/8åˆ†è¾¨ç‡): {t2.shape}")
    print(f"  T3 (1/16åˆ†è¾¨ç‡): {t3.shape}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶ä¿æŒä¸å˜
    assert t1.shape == x_1_4.shape, "1/4åˆ†è¾¨ç‡å½¢çŠ¶ä¸åŒ¹é…"
    assert t2.shape == x_1_8.shape, "1/8åˆ†è¾¨ç‡å½¢çŠ¶ä¸åŒ¹é…"
    assert t3.shape == x_1_16.shape, "1/16åˆ†è¾¨ç‡å½¢çŠ¶ä¸åŒ¹é…"
    
    print("\nâœ“ CRF-3èåˆæ¨¡å—æµ‹è¯•é€šè¿‡")
    
    # å‚æ•°ç»Ÿè®¡
    fusion_params = sum(p.numel() for p in fusion.parameters())
    print(f"\nCRF-3èåˆæ¨¡å—å‚æ•°é‡: {fusion_params:,}")
    
    # è¯¦ç»†ç»Ÿè®¡å„è·¯å¾„å‚æ•°
    print("\nèåˆè·¯å¾„å‚æ•°ç»Ÿè®¡:")
    
    # åˆ°1/4åˆ†æ”¯çš„è·¯å¾„
    to_1_4_params = sum(p.numel() for name, p in fusion.named_parameters() 
                        if 'to_1_4' in name or 'smooth_1_4' in name)
    print(f"  æ±‡åˆ°1/4åˆ†æ”¯: {to_1_4_params:,}")
    
    # åˆ°1/8åˆ†æ”¯çš„è·¯å¾„
    to_1_8_params = sum(p.numel() for name, p in fusion.named_parameters() 
                        if 'to_1_8' in name or 'smooth_1_8' in name)
    print(f"  æ±‡åˆ°1/8åˆ†æ”¯: {to_1_8_params:,}")
    
    # åˆ°1/16åˆ†æ”¯çš„è·¯å¾„
    to_1_16_params = sum(p.numel() for name, p in fusion.named_parameters() 
                         if 'to_1_16' in name or 'smooth_1_16' in name)
    print(f"  æ±‡åˆ°1/16åˆ†æ”¯: {to_1_16_params:,}")
    
    print("=" * 60)


def test_multi_resolution_consistency():
    """æµ‹è¯•å¤šåˆ†è¾¨ç‡ä¸€è‡´æ€§"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•å¤šåˆ†è¾¨ç‡ä¸€è‡´æ€§")
    print("=" * 60)
    
    # åˆ›å»ºStage3
    stage3_config = get_stage3_config()
    stage3 = create_stage3(stage3_config)
    stage3.eval()
    
    # åˆ›å»ºå…·æœ‰ç‰¹å®šæ¨¡å¼çš„è¾“å…¥ï¼Œä»¥éªŒè¯åˆ†è¾¨ç‡å¤„ç†
    batch_size = 1
    
    # åˆ›å»ºå…·æœ‰æ¸å˜æ¨¡å¼çš„è¾“å…¥
    y1 = torch.ones(batch_size, 32, 64, 128) * 0.5
    y2 = torch.ones(batch_size, 64, 32, 64) * 0.5
    
    with torch.no_grad():
        outputs = stage3([y1, y2])
    
    print("\nåˆ†è¾¨ç‡ä¸‹é‡‡æ ·éªŒè¯:")
    print(f"  1/4 â†’ 1/8: {outputs[0].shape} â†’ {outputs[1].shape}")
    print(f"  1/8 â†’ 1/16: {outputs[1].shape} â†’ {outputs[2].shape}")
    
    # éªŒè¯é€šé“æ•°é€’å¢
    channels = [out.shape[1] for out in outputs]
    assert channels == stage3_config['channels'], f"é€šé“æ•°ä¸åŒ¹é…: {channels} != {stage3_config['channels']}"
    
    print("\nâœ“ å¤šåˆ†è¾¨ç‡ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡")
    print("=" * 60)


if __name__ == "__main__":
    # æµ‹è¯•Stage3ä¸»æ¨¡å—
    test_stage3()
    
    # æµ‹è¯•è·¨åˆ†è¾¨ç‡èåˆæ¨¡å—
    test_cross_resolution_fusion3()
    
    # æµ‹è¯•å¤šåˆ†è¾¨ç‡ä¸€è‡´æ€§
    test_multi_resolution_consistency()
    
    print("\n" + "=" * 60)
    print("ğŸ¯ æ‰€æœ‰Stage3æµ‹è¯•é€šè¿‡! âœ“")
    print("=" * 60)