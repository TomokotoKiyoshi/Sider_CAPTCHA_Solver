# -*- coding: utf-8 -*-
"""
Stage5 LiteFPNæ¨¡å—æµ‹è¯•è„šæœ¬
æµ‹è¯•LiteFPNçš„é…ç½®åŠ è½½ã€å‰å‘ä¼ æ’­å’Œç‰¹å¾é‡‘å­—å¡”èåˆ
"""
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

import torch
from src.models.stage5_lite_fpn import create_stage5_lite_fpn, LiteFPN, FusionModule
from src.models.config_loader import get_stage5_lite_fpn_config


def test_stage5_lite_fpn():
    """æµ‹è¯•Stage5 LiteFPNæ¨¡å—"""
    print("=" * 70)
    print("æµ‹è¯•Stage5 LiteFPNæ¨¡å— - è½»é‡çº§ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ")
    print("=" * 70)
    
    # åŠ è½½é…ç½®
    config = get_stage5_lite_fpn_config()
    print("\né…ç½®ä¿¡æ¯:")
    print(f"  è¾“å…¥é€šé“: {config['in_channels']}")
    print(f"  FPNé€šé“: {config['fpn_channels']}")
    print(f"  èåˆç±»å‹: {config['fusion_type']}")
    print(f"  è¿”å›é‡‘å­—å¡”: {config['return_pyramid']}")
    
    # åˆ›å»ºæ¨¡å—
    lite_fpn = create_stage5_lite_fpn(config)
    lite_fpn.eval()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥ï¼ˆæ¨¡æ‹ŸStage4çš„è¾“å‡ºï¼‰
    batch_size = 2
    features = [
        torch.randn(batch_size, 32, 64, 128),   # B1 (1/4åˆ†è¾¨ç‡)
        torch.randn(batch_size, 64, 32, 64),    # B2 (1/8åˆ†è¾¨ç‡)
        torch.randn(batch_size, 128, 16, 32),   # B3 (1/16åˆ†è¾¨ç‡)
        torch.randn(batch_size, 256, 8, 16),    # B4 (1/32åˆ†è¾¨ç‡)
    ]
    
    print("\nè¾“å…¥å¼ é‡ï¼ˆæ¥è‡ªStage4ï¼‰:")
    for i, feat in enumerate(features, 1):
        print(f"  B{i} (1/{4*(2**(i-1))}åˆ†è¾¨ç‡): {feat.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = lite_fpn(features)
    
    print("\nè¾“å‡ºä¿¡æ¯:")
    if isinstance(output, tuple):
        hf, pyramid = output
        print(f"  ä¸»ç‰¹å¾ Hf (1/4åˆ†è¾¨ç‡): {hf.shape}")
        print(f"  ä¸­é—´é‡‘å­—å¡”ç‰¹å¾:")
        for i, feat in enumerate(pyramid):
            scale = 8 * (2**i)
            print(f"    P{i+3}_td (1/{scale}åˆ†è¾¨ç‡): {feat.shape}")
    else:
        print(f"  ä¸»ç‰¹å¾ Hf (1/4åˆ†è¾¨ç‡): {output.shape}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_shape = (batch_size, config['fpn_channels'], 64, 128)
    actual_shape = output.shape if not isinstance(output, tuple) else output[0].shape
    assert actual_shape == expected_shape, \
        f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: æœŸæœ›{expected_shape}, å®é™…{actual_shape}"
    
    print("\nâœ“ è¾“å‡ºå½¢çŠ¶éªŒè¯é€šè¿‡")
    
    # å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in lite_fpn.parameters())
    trainable_params = sum(p.numel() for p in lite_fpn.parameters() if p.requires_grad)
    
    print("\nå‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)")
    
    # åˆ†æ¨¡å—å‚æ•°ç»Ÿè®¡
    lateral_params = sum(p.numel() for name, p in lite_fpn.named_parameters() if 'lateral' in name)
    smooth_params = sum(p.numel() for name, p in lite_fpn.named_parameters() if 'smooth' in name)
    fusion_params = sum(p.numel() for name, p in lite_fpn.named_parameters() if 'fuse' in name)
    
    print("\nåˆ†æ¨¡å—å‚æ•°ç»Ÿè®¡:")
    print(f"  ä¾§è¿æ¥æ¨¡å—: {lateral_params:,}")
    print(f"  å¹³æ»‘å·ç§¯: {smooth_params:,}")
    print(f"  èåˆæ¨¡å—: {fusion_params:,}")
    
    print("\nâœ“ Stage5 LiteFPNæ¨¡å—æµ‹è¯•å®Œæˆ")
    print("=" * 70)


def test_fusion_modules():
    """æµ‹è¯•ä¸åŒçš„èåˆæ¨¡å—"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•èåˆæ¨¡å—")
    print("=" * 70)
    
    channels = 128
    batch_size = 2
    height, width = 32, 64
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    a = torch.randn(batch_size, channels, height, width)
    b = torch.randn(batch_size, channels, height, width)
    
    # æµ‹è¯•ä¸åŒèåˆç±»å‹
    fusion_types = ['add', 'weighted', 'attention']
    
    for fusion_type in fusion_types:
        print(f"\næµ‹è¯• {fusion_type} èåˆ:")
        
        fusion_module = FusionModule(channels, fusion_type)
        fusion_module.eval()
        
        with torch.no_grad():
            output = fusion_module(a, b)
        
        assert output.shape == a.shape, \
            f"{fusion_type}èåˆè¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"
        
        # å‚æ•°ç»Ÿè®¡
        params = sum(p.numel() for p in fusion_module.parameters())
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  å‚æ•°é‡: {params:,}")
        
        if fusion_type == 'weighted':
            # æ£€æŸ¥æƒé‡å‚æ•°
            print(f"  æƒé‡a: {fusion_module.weight_a.item():.4f}")
            print(f"  æƒé‡b: {fusion_module.weight_b.item():.4f}")
        
        print(f"  âœ“ {fusion_type}èåˆæµ‹è¯•é€šè¿‡")
    
    print("\n" + "=" * 70)


def test_resolution_consistency():
    """æµ‹è¯•åˆ†è¾¨ç‡ä¸€è‡´æ€§"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•åˆ†è¾¨ç‡ä¸€è‡´æ€§")
    print("=" * 70)
    
    # åŠ è½½é…ç½®
    config = get_stage5_lite_fpn_config()
    config['return_pyramid'] = True  # è¿”å›ä¸­é—´é‡‘å­—å¡”
    
    # åˆ›å»ºæ¨¡å—
    lite_fpn = LiteFPN(**config)
    lite_fpn.eval()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 1
    features = [
        torch.randn(batch_size, 32, 64, 128),   # 1/4åˆ†è¾¨ç‡
        torch.randn(batch_size, 64, 32, 64),    # 1/8åˆ†è¾¨ç‡
        torch.randn(batch_size, 128, 16, 32),   # 1/16åˆ†è¾¨ç‡
        torch.randn(batch_size, 256, 8, 16),    # 1/32åˆ†è¾¨ç‡
    ]
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        hf, pyramid = lite_fpn(features)
    
    print("\nåˆ†è¾¨ç‡éªŒè¯:")
    
    # ä¸»ç‰¹å¾
    print(f"  ä¸»ç‰¹å¾ Hf: {hf.shape} (æœŸæœ›1/4åˆ†è¾¨ç‡: 64Ã—128)")
    assert hf.shape[2:] == (64, 128), "ä¸»ç‰¹å¾åˆ†è¾¨ç‡ä¸æ­£ç¡®"
    
    # ä¸­é—´é‡‘å­—å¡”
    expected_resolutions = [(32, 64), (16, 32), (8, 16)]
    pyramid_names = ['P3_td', 'P4_td', 'P5']
    
    for name, feat, expected in zip(pyramid_names, pyramid, expected_resolutions):
        print(f"  {name}: {feat.shape} (æœŸæœ›: {expected})")
        assert feat.shape[2:] == expected, f"{name}åˆ†è¾¨ç‡ä¸æ­£ç¡®"
    
    print("\nâœ“ åˆ†è¾¨ç‡ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡")
    print("=" * 70)


def test_complete_pipeline():
    """æµ‹è¯•å®Œæ•´çš„æ•°æ®æµï¼ˆStage4 â†’ Stage5ï¼‰"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•å®Œæ•´æ•°æ®æµï¼ˆStage4 â†’ Stage5 LiteFPNï¼‰")
    print("=" * 70)
    
    # å¯¼å…¥Stage4
    from src.models.stage4 import create_stage4
    from src.models.config_loader import get_stage4_config, get_stage3_config
    from src.models.stage3 import create_stage3
    
    # åˆ›å»ºStage3è¾“å‡ºï¼ˆæ¨¡æ‹Ÿï¼‰
    batch_size = 2
    stage3_output = [
        torch.randn(batch_size, 32, 64, 128),   # T1
        torch.randn(batch_size, 64, 32, 64),    # T2
        torch.randn(batch_size, 128, 16, 32),   # T3
    ]
    
    print("Stage3è¾“å‡º:")
    for i, feat in enumerate(stage3_output, 1):
        print(f"  T{i}: {feat.shape}")
    
    # Stage4å¤„ç†
    stage4_config = get_stage4_config()
    stage4 = create_stage4(stage4_config)
    stage4.eval()
    
    with torch.no_grad():
        stage4_output = stage4(stage3_output)
    
    print("\nStage4è¾“å‡ºï¼ˆä¸»å¹²æœ€ç»ˆï¼‰:")
    for i, feat in enumerate(stage4_output, 1):
        print(f"  B{i}: {feat.shape}")
    
    # Stage5 LiteFPNå¤„ç†
    stage5_config = get_stage5_lite_fpn_config()
    stage5 = create_stage5_lite_fpn(stage5_config)
    stage5.eval()
    
    with torch.no_grad():
        final_output = stage5(stage4_output)
    
    print("\nStage5 LiteFPNè¾“å‡º:")
    print(f"  ä¸»ç‰¹å¾ Hf: {final_output.shape}")
    print(f"  é€šé“æ•°: {final_output.shape[1]} (ç»Ÿä¸€åˆ°128)")
    print(f"  åˆ†è¾¨ç‡: 1/4 (64Ã—128)")
    
    # éªŒè¯æœ€ç»ˆè¾“å‡º
    assert final_output.shape == (batch_size, 128, 64, 128), \
        "æœ€ç»ˆè¾“å‡ºå½¢çŠ¶ä¸æ­£ç¡®"
    
    print("\nâœ“ å®Œæ•´æ•°æ®æµæµ‹è¯•é€šè¿‡")
    print("=" * 70)


if __name__ == "__main__":
    # æµ‹è¯•Stage5 LiteFPNä¸»æ¨¡å—
    test_stage5_lite_fpn()
    
    # æµ‹è¯•èåˆæ¨¡å—
    test_fusion_modules()
    
    # æµ‹è¯•åˆ†è¾¨ç‡ä¸€è‡´æ€§
    test_resolution_consistency()
    
    # æµ‹è¯•å®Œæ•´ç®¡é“
    test_complete_pipeline()
    
    print("\n" + "=" * 70)
    print("ğŸ¯ æ‰€æœ‰Stage5 LiteFPNæµ‹è¯•é€šè¿‡! âœ“")
    print("ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œæ„å»ºå®Œæˆ!")
    print("=" * 70)