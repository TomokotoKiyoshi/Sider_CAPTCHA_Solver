# Lite-HRNet-18+LiteFPN è®­ç»ƒä¸å¯è§†åŒ–æŒ‡å—

## ğŸ“‹ ç›®å½•

- [æ¨¡å‹æ¶æ„æ¦‚è¿°](#æ¨¡å‹æ¶æ„æ¦‚è¿°)
- [è®­ç»ƒç›®æ ‡ä¸æŒ‡æ ‡](#è®­ç»ƒç›®æ ‡ä¸æŒ‡æ ‡)
- [è®­ç»ƒé…ç½®](#è®­ç»ƒé…ç½®)
- [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
- [æ¨¡å‹ä¿å­˜ç­–ç•¥](#æ¨¡å‹ä¿å­˜ç­–ç•¥)
- [å¯è§†åŒ–ç›‘æ§](#å¯è§†åŒ–ç›‘æ§)
- [TensorBoard ç›‘æ§é¢æ¿](#tensorboard-ç›‘æ§é¢æ¿)
- [ç»“æœè¯„ä¼°ä¸åˆ†æ](#ç»“æœè¯„ä¼°ä¸åˆ†æ)
- [æ•…éšœæ’æŸ¥æŒ‡å—](#æ•…éšœæ’æŸ¥æŒ‡å—)

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„æ¦‚è¿°

### æ¨¡å‹è§„æ ¼

- **æ¨¡å‹åç§°**: Lite-HRNet-18+LiteFPN
- **å‚æ•°é‡**: 3.28M (ç›®æ ‡3.5Mï¼Œè¯¯å·®-6.1%)
- **æ¨¡å‹å¤§å°**: 
  - FP32: 12.53 MB
  - FP16: 6.27 MB
  - INT8: 3.13 MB
- **æ¨ç†é€Ÿåº¦**: 
  - CPU (i5-1240P): 12-14ms
  - GPU (RTX 3050): <3ms

### æ¶æ„ç»„æˆ

```
è¾“å…¥ [B, 4, 256, 512] (RGB + padding mask)
    â†“
Stem (Stage1): 20,480 å‚æ•° (0.6%)
    â†“
Stage2 (åŒåˆ†æ”¯): 100,864 å‚æ•° (3.1%)
    â†“
Stage3 (ä¸‰åˆ†æ”¯): 512,416 å‚æ•° (15.6%)
    â†“
Stage4 (å››åˆ†æ”¯): 1,701,568 å‚æ•° (51.8%)
    â†“
LiteFPN (Stage5): 653,318 å‚æ•° (19.9%)
    â†“
DualHead (Stage6): 296,200 å‚æ•° (9.0%)
    â†“
è¾“å‡º:
- heatmap_gap [B, 1, 64, 128]
- heatmap_slider [B, 1, 64, 128]
- offset_gap [B, 2, 64, 128]
- offset_slider [B, 2, 64, 128]
- angle [B, 2, 64, 128] (å¯é€‰)
```

---

## ğŸ¯ è®­ç»ƒç›®æ ‡ä¸æŒ‡æ ‡

### ä¸»è¦ç›®æ ‡

- **MAE (Mean Absolute Error)**: < 5px
- **ç²¾åº¦@5px**: > 98%
- **æ¨ç†é€Ÿåº¦**: CPU < 15ms

### è¯„ä¼°æŒ‡æ ‡ä½“ç³»

| æŒ‡æ ‡          | ç›®æ ‡å€¼     | è®¡ç®—æ–¹æ³•                         | é‡è¦æ€§       |
| ----------- | ------- | ---------------------------- | --------- |
| **MAE**     | < 5px   | `mean(                       | pred - gt |
| **ç²¾åº¦@5px**  | > 98%   | `sum(distance < 5) / total`  | ğŸ”´ å…³é”®     |
| **ç²¾åº¦@3px**  | > 85%   | `sum(distance < 3) / total`  | ğŸŸ¡ é‡è¦     |
| **ç²¾åº¦@10px** | > 99.5% | `sum(distance < 10) / total` | ğŸŸ¢ å‚è€ƒ     |
| **ç¼ºå£MAE**   | < 5px   | ä»…è®¡ç®—ç¼ºå£ä½ç½®è¯¯å·®                    | ğŸ”´ å…³é”®     |
| **æ»‘å—MAE**   | < 5px   | ä»…è®¡ç®—æ»‘å—ä½ç½®è¯¯å·®                    | ğŸ”´ å…³é”®     |
| **è§’åº¦è¯¯å·®**    | < 2Â°    | æ—‹è½¬ç¼ºå£çš„è§’åº¦è¯¯å·®                    | ğŸŸ¢ å‚è€ƒ     |

---

## âš™ï¸ è®­ç»ƒé…ç½®

### è¶…å‚æ•°è®¾ç½®

```yaml
# config/training_config.yaml
model:
  name: "Lite-HRNet-18+LiteFPN"
  input_channels: 4  # RGB + padding mask

training:
  # ä¼˜åŒ–å™¨é…ç½®
  optimizer:
    type: "AdamW"
    lr: 3e-4  # åŸºç¡€å­¦ä¹ ç‡
    betas: [0.9, 0.999]
    eps: 1e-8
    weight_decay: 0.05

  # å­¦ä¹ ç‡è°ƒåº¦
  scheduler:
    type: "CosineAnnealingWarmRestarts"
    T_0: 10  # ç¬¬ä¸€ä¸ªå‘¨æœŸçš„epochæ•°
    T_mult: 2  # å‘¨æœŸå€å¢å› å­
    eta_min: 3e-6  # æœ€å°å­¦ä¹ ç‡
    warmup_epochs: 5  # é¢„çƒ­epochæ•°
    warmup_lr: 1e-5  # é¢„çƒ­èµ·å§‹å­¦ä¹ ç‡

  # è®­ç»ƒå‚æ•°
  batch_size: 16  # åŸºç¡€æ‰¹æ¬¡å¤§å°
  num_epochs: 160
  gradient_clip: 1.0  # æ¢¯åº¦è£å‰ª

  # EMAé…ç½®
  ema:
    enabled: true
    decay: 0.9997
    update_interval: 1

  # æ•°æ®å¢å¼º
  augmentation:
    # å‡ ä½•å˜æ¢
    random_rotate: [-5, 5]  # åº¦
    random_scale: [0.9, 1.1]
    random_translate: [-10, 10]  # åƒç´ 

    # é¢œè‰²å¢å¼º
    brightness: 0.2
    contrast: 0.2
    saturation: 0.2
    hue: 0.1

    # å™ªå£°ä¸æ¨¡ç³Š
    gaussian_noise: 0.01
    gaussian_blur: [0, 2]

    # æ··æ·†å¢å¼ºï¼ˆä¸“é—¨é’ˆå¯¹éªŒè¯ç ï¼‰
    confusing_gap_prob: 0.3
    perlin_noise_prob: 0.2
    gap_rotation_prob: 0.1
    gap_highlight_prob: 0.1

loss:
  # çƒ­åŠ›å›¾æŸå¤± (Focal Loss)
  heatmap:
    type: "FocalLoss"
    alpha: 2
    beta: 4
    weight: 1.0

  # åç§»æŸå¤± (L1 Loss)
  offset:
    type: "L1Loss"
    weight: 1.0

  # è§’åº¦æŸå¤± (å¯é€‰)
  angle:
    type: "L1Loss"
    weight: 0.5

validation:
  interval: 1  # æ¯ä¸ªepochéªŒè¯ä¸€æ¬¡
  save_best: true
  metrics: ["mae", "accuracy@5px", "accuracy@3px"]

checkpoints:
  save_dir: "checkpoints/1.1.0"
  save_interval: 1  # æ¯ä¸ªepochä¿å­˜
  keep_last: 5  # ä¿ç•™æœ€è¿‘5ä¸ªcheckpoint
  save_best: true  # ä¿å­˜æœ€ä½³æ¨¡å‹
```

### æ‰¹æ¬¡å¤§å°è‡ªé€‚åº”

```python
# æ ¹æ®GPUå†…å­˜è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°
def get_adaptive_batch_size(gpu_memory_gb):
    if gpu_memory_gb >= 24:
        return 64
    elif gpu_memory_gb >= 16:
        return 32
    elif gpu_memory_gb >= 8:
        return 16
    else:
        return 8
```

---

## ğŸš€ è®­ç»ƒæµç¨‹

### 1. æ•°æ®å‡†å¤‡é˜¶æ®µ

```bash
# ç”ŸæˆéªŒè¯ç æ•°æ®é›†
python scripts/generate_captchas.py \
    --num_images 2000 \
    --output_dir data/captchas \
    --enable_confusing_gaps \
    --enable_rotation \
    --enable_perlin_noise

# åˆ’åˆ†æ•°æ®é›† (9:1)
python scripts/split_dataset.py \
    --input_dir data/captchas \
    --train_ratio 0.9 \
    --seed 42
```

### 2. è®­ç»ƒå¯åŠ¨

```bash
# åŸºç¡€è®­ç»ƒ
python scripts/training/train.py \
    --config config/training_config.yaml \
    --resume_from checkpoints/1.1.0/last.pth

# å¤šGPUè®­ç»ƒ
python -m torch.distributed.launch \
    --nproc_per_node=2 \
    scripts/training/train.py \
    --config config/training_config.yaml \
    --distributed

# æ··åˆç²¾åº¦è®­ç»ƒ (èŠ‚çœå†…å­˜ï¼ŒåŠ é€Ÿè®­ç»ƒ)
python scripts/training/train.py \
    --config config/training_config.yaml \
    --amp \
    --amp_level O1
```

### 3. è®­ç»ƒç›‘æ§

```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir runs/1.1.0 --port 6006

# å®æ—¶æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f logs/training_$(date +%Y%m%d).log
```

---

## ğŸ’¾ æ¨¡å‹ä¿å­˜ç­–ç•¥

### ä¿å­˜è·¯å¾„ç»“æ„

```
checkpoints/1.1.0/
â”œâ”€â”€ epoch_001.pth      # æ¯ä¸ªepochçš„checkpoint
â”œâ”€â”€ epoch_002.pth
â”œâ”€â”€ ...
â”œâ”€â”€ epoch_160.pth
â”œâ”€â”€ best_model.pth     # æµ‹è¯•é›†æœ€ä½³æ¨¡å‹
â”œâ”€â”€ last.pth          # æœ€æ–°æ¨¡å‹ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
â””â”€â”€ config.yaml       # è®­ç»ƒé…ç½®å¤‡ä»½
```

### Checkpoint å†…å®¹

```python
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict(),
    'ema_state_dict': ema.state_dict() if use_ema else None,
    'metrics': {
        'train_loss': train_loss,
        'val_loss': val_loss,
        'val_mae': val_mae,
        'val_acc_5px': val_acc_5px,
        'val_acc_3px': val_acc_3px,
        'best_mae': best_mae,
    },
    'config': config,
    'random_state': {
        'torch': torch.get_rng_state(),
        'numpy': np.random.get_state(),
        'random': random.getstate(),
    }
}
```

### ä¿å­˜è§¦å‘æ¡ä»¶

1. **æ¯ä¸ªEpochç»“æŸ**: ä¿å­˜ä¸º `epoch_XXX.pth`
2. **éªŒè¯é›†æœ€ä½³**: MAEæœ€ä½æ—¶ä¿å­˜ä¸º `best_model.pth`
3. **è®­ç»ƒä¸­æ–­**: è‡ªåŠ¨ä¿å­˜ä¸º `last.pth`
4. **å®šæœŸä¿å­˜**: æ¯30åˆ†é’Ÿè‡ªåŠ¨ä¿å­˜ä¸€æ¬¡

---

## ğŸ“Š å¯è§†åŒ–ç›‘æ§

### 1. è®­ç»ƒæ›²çº¿å¯è§†åŒ–

```python
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_theme(style="darkgrid")
plt.rcParams['font.sans-serif'] = ['SimHei']  # ä¸­æ–‡æ”¯æŒ
plt.rcParams['axes.unicode_minus'] = False

# åˆ›å»º2x2å­å›¾
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Lossæ›²çº¿
axes[0, 0].plot(epochs, train_losses, 'b-', label='è®­ç»ƒé›†Loss', linewidth=2)
axes[0, 0].plot(epochs, val_losses, 'r-', label='éªŒè¯é›†Loss', linewidth=2)
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].set_title('æŸå¤±å‡½æ•°å˜åŒ–æ›²çº¿')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 2. MAEæ›²çº¿
axes[0, 1].plot(epochs, train_mae, 'b-', label='è®­ç»ƒé›†MAE', linewidth=2)
axes[0, 1].plot(epochs, val_mae, 'r-', label='éªŒè¯é›†MAE', linewidth=2)
axes[0, 1].axhline(y=5, color='g', linestyle='--', label='ç›®æ ‡: 5px')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('MAE (pixels)')
axes[0, 1].set_title('å¹³å‡ç»å¯¹è¯¯å·®å˜åŒ–æ›²çº¿')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# 3. ç²¾åº¦æ›²çº¿
axes[1, 0].plot(epochs, acc_3px, 'g-', label='ç²¾åº¦@3px', linewidth=2)
axes[1, 0].plot(epochs, acc_5px, 'b-', label='ç²¾åº¦@5px', linewidth=2)
axes[1, 0].plot(epochs, acc_10px, 'r-', label='ç²¾åº¦@10px', linewidth=2)
axes[1, 0].axhline(y=98, color='k', linestyle='--', label='ç›®æ ‡: 98%')
axes[1, 0].set_xlabel('Epoch')
axes[1, 0].set_ylabel('Accuracy (%)')
axes[1, 0].set_title('ä¸åŒé˜ˆå€¼ä¸‹çš„ç²¾åº¦å˜åŒ–')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# 4. å­¦ä¹ ç‡å˜åŒ–
axes[1, 1].plot(epochs, learning_rates, 'orange', linewidth=2)
axes[1, 1].set_xlabel('Epoch')
axes[1, 1].set_ylabel('Learning Rate')
axes[1, 1].set_title('å­¦ä¹ ç‡è°ƒåº¦')
axes[1, 1].set_yscale('log')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('results/training_curves.png', dpi=300, bbox_inches='tight')
plt.show()
```

### 2. è¯¯å·®åˆ†å¸ƒåˆ†æ

```python
# è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
plt.figure(figsize=(12, 5))

# ç¼ºå£è¯¯å·®åˆ†å¸ƒ
plt.subplot(1, 2, 1)
plt.hist(gap_errors, bins=50, density=True, alpha=0.7, color='blue', edgecolor='black')
plt.axvline(x=5, color='r', linestyle='--', label='ç›®æ ‡é˜ˆå€¼: 5px')
plt.xlabel('è¯¯å·® (pixels)')
plt.ylabel('æ¦‚ç‡å¯†åº¦')
plt.title(f'ç¼ºå£ä½ç½®è¯¯å·®åˆ†å¸ƒ\nMAE: {gap_mae:.2f}px')
plt.legend()

# æ»‘å—è¯¯å·®åˆ†å¸ƒ
plt.subplot(1, 2, 2)
plt.hist(slider_errors, bins=50, density=True, alpha=0.7, color='green', edgecolor='black')
plt.axvline(x=5, color='r', linestyle='--', label='ç›®æ ‡é˜ˆå€¼: 5px')
plt.xlabel('è¯¯å·® (pixels)')
plt.ylabel('æ¦‚ç‡å¯†åº¦')
plt.title(f'æ»‘å—ä½ç½®è¯¯å·®åˆ†å¸ƒ\nMAE: {slider_mae:.2f}px')
plt.legend()

plt.tight_layout()
plt.savefig('results/error_distribution.png', dpi=300, bbox_inches='tight')
```

### 3. æ··æ·†çŸ©é˜µï¼ˆé’ˆå¯¹å›°éš¾æ ·æœ¬ï¼‰

```python
# æŒ‰è¯¯å·®ç­‰çº§åˆ†ç±»çš„æ··æ·†çŸ©é˜µ
error_ranges = ['0-3px', '3-5px', '5-10px', '>10px']
confusion_matrix = np.array([
    [850, 50, 20, 5],   # ç®€å•æ ·æœ¬
    [30, 40, 10, 2],    # æ··æ·†ç¼ºå£
    [10, 8, 15, 3],     # æ—‹è½¬ç¼ºå£
    [2, 1, 3, 1]        # å™ªå£°æ ·æœ¬
])

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='YlOrRd',
            xticklabels=error_ranges,
            yticklabels=['ç®€å•', 'æ··æ·†ç¼ºå£', 'æ—‹è½¬ç¼ºå£', 'å™ªå£°æ ·æœ¬'])
plt.title('ä¸åŒæ ·æœ¬ç±»å‹çš„è¯¯å·®åˆ†å¸ƒ')
plt.xlabel('è¯¯å·®èŒƒå›´')
plt.ylabel('æ ·æœ¬ç±»å‹')
plt.savefig('results/confusion_matrix.png', dpi=300, bbox_inches='tight')
```

---

## ğŸ“ˆ TensorBoard ç›‘æ§é¢æ¿

### 1. Scalarsï¼ˆæ ‡é‡ç›‘æ§ï¼‰

```python
# åœ¨è®­ç»ƒä»£ç ä¸­æ·»åŠ 
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter(f'runs/1.1.0/exp_{timestamp}')

# æ¯ä¸ªstepè®°å½•
writer.add_scalar('Loss/train', train_loss, global_step)
writer.add_scalar('Loss/val', val_loss, global_step)
writer.add_scalar('Metrics/MAE', mae, global_step)
writer.add_scalar('Metrics/Accuracy_5px', acc_5px, global_step)
writer.add_scalar('Metrics/Accuracy_3px', acc_3px, global_step)
writer.add_scalar('Learning_Rate', current_lr, global_step)

# åˆ†ç»„è®°å½•
writer.add_scalars('Loss_Comparison', {
    'train': train_loss,
    'validation': val_loss
}, global_step)

writer.add_scalars('MAE_Components', {
    'gap': gap_mae,
    'slider': slider_mae,
    'total': total_mae
}, global_step)
```

### 2. Imagesï¼ˆå›¾åƒç›‘æ§ï¼‰

```python
# å¯è§†åŒ–é¢„æµ‹ç»“æœ
def visualize_predictions(images, targets, predictions, writer, step):
    batch_size = min(4, images.size(0))  # æ˜¾ç¤º4ä¸ªæ ·æœ¬

    fig, axes = plt.subplots(batch_size, 3, figsize=(12, batch_size*3))

    for i in range(batch_size):
        # åŸå›¾
        img = images[i].cpu().numpy().transpose(1, 2, 0)
        axes[i, 0].imshow(img[:, :, :3])  # åªæ˜¾ç¤ºRGBé€šé“
        axes[i, 0].set_title('è¾“å…¥å›¾åƒ')

        # çœŸå®çƒ­åŠ›å›¾
        gt_heatmap = targets['heatmap_gap'][i].cpu().numpy()
        axes[i, 1].imshow(gt_heatmap[0], cmap='hot')
        axes[i, 1].set_title('çœŸå®çƒ­åŠ›å›¾')

        # é¢„æµ‹çƒ­åŠ›å›¾
        pred_heatmap = predictions['heatmap_gap'][i].cpu().numpy()
        axes[i, 2].imshow(pred_heatmap[0], cmap='hot')
        axes[i, 2].set_title('é¢„æµ‹çƒ­åŠ›å›¾')

        # æ·»åŠ åæ ‡ç‚¹
        gt_coord = targets['gap_coords'][i].cpu().numpy()
        pred_coord = predictions['gap_coords'][i].cpu().numpy()

        axes[i, 0].plot(gt_coord[0], gt_coord[1], 'go', markersize=10, label='çœŸå®')
        axes[i, 0].plot(pred_coord[0], pred_coord[1], 'rx', markersize=10, label='é¢„æµ‹')
        axes[i, 0].legend()

    plt.tight_layout()
    writer.add_figure('Predictions', fig, step)
    plt.close()

# æ¯10ä¸ªepochå¯è§†åŒ–ä¸€æ¬¡
if epoch % 10 == 0:
    visualize_predictions(images, targets, outputs, writer, epoch)
```

### 3. Histogramsï¼ˆç›´æ–¹å›¾ç›‘æ§ï¼‰

```python
# ç›‘æ§æƒé‡å’Œæ¢¯åº¦åˆ†å¸ƒ
for name, param in model.named_parameters():
    if param.requires_grad:
        writer.add_histogram(f'Weights/{name}', param.data, epoch)
        if param.grad is not None:
            writer.add_histogram(f'Gradients/{name}', param.grad, epoch)

# ç›‘æ§æ¿€æ´»å€¼åˆ†å¸ƒ
def hook_fn(module, input, output):
    writer.add_histogram(f'Activations/{module.__class__.__name__}', 
                         output.detach(), epoch)

# æ³¨å†Œé’©å­
for module in model.modules():
    if isinstance(module, (nn.Conv2d, nn.Linear)):
        module.register_forward_hook(hook_fn)
```

### 4. Graphï¼ˆæ¨¡å‹ç»“æ„ï¼‰

```python
# å¯è§†åŒ–æ¨¡å‹ç»“æ„
dummy_input = torch.randn(1, 4, 256, 512).to(device)
writer.add_graph(model, dummy_input)
```

### 5. Custom Scalarsï¼ˆè‡ªå®šä¹‰é¢æ¿ï¼‰

```python
# åˆ›å»ºè‡ªå®šä¹‰ç›‘æ§é¢æ¿
layout = {
    "è®­ç»ƒç›‘æ§": {
        "æŸå¤±å‡½æ•°": ["Multiline", ["Loss/train", "Loss/val"]],
        "å­¦ä¹ ç‡": ["Multiline", ["Learning_Rate"]],
    },
    "æ€§èƒ½æŒ‡æ ‡": {
        "MAE": ["Multiline", ["Metrics/MAE", "MAE_Components/gap", "MAE_Components/slider"]],
        "ç²¾åº¦": ["Multiline", ["Metrics/Accuracy_3px", "Metrics/Accuracy_5px"]],
    },
    "æ¨¡å‹åˆ†æ": {
        "æ¢¯åº¦èŒƒæ•°": ["Multiline", ["Gradients/norm"]],
        "æƒé‡æ›´æ–°": ["Multiline", ["Weights/update_ratio"]],
    }
}

writer.add_custom_scalars(layout)
```

### 6. PR Curvesï¼ˆç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿ï¼‰

```python
# è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„PRæ›²çº¿
def compute_pr_curve(predictions, targets, thresholds):
    precisions = []
    recalls = []

    for threshold in thresholds:
        pred_positive = predictions > threshold
        true_positive = pred_positive & targets

        precision = true_positive.sum() / pred_positive.sum()
        recall = true_positive.sum() / targets.sum()

        precisions.append(precision)
        recalls.append(recall)

    return precisions, recalls

# æ·»åŠ åˆ°TensorBoard
writer.add_pr_curve('PR_Curve', targets, predictions, epoch)
```

---

## ğŸ¯ ç»“æœè¯„ä¼°ä¸åˆ†æ

### 1. æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡

```python
# æµ‹è¯•é›†å®Œæ•´è¯„ä¼°
def evaluate_final_model(model, test_loader):
    results = {
        'mae': {'gap': [], 'slider': [], 'total': []},
        'accuracy': {'3px': 0, '5px': 0, '10px': 0},
        'error_distribution': [],
        'inference_time': [],
        'difficult_samples': []
    }

    model.eval()
    with torch.no_grad():
        for batch in test_loader:
            # é¢„æµ‹
            outputs = model(batch['image'])
            predictions = model.decode_predictions(outputs)

            # è®¡ç®—è¯¯å·®
            gap_error = torch.abs(predictions['gap_coords'] - batch['gap_coords'])
            slider_error = torch.abs(predictions['slider_coords'] - batch['slider_coords'])

            # ç»Ÿè®¡ç»“æœ
            results['mae']['gap'].extend(gap_error.mean(dim=1).cpu().numpy())
            results['mae']['slider'].extend(slider_error.mean(dim=1).cpu().numpy())

            # å›°éš¾æ ·æœ¬è¯†åˆ«
            if batch.get('has_confusing_gap', False):
                results['difficult_samples'].append({
                    'type': 'confusing_gap',
                    'error': gap_error.mean().item()
                })

    return results
```

### 2. ç»“æœæŠ¥å‘Šç”Ÿæˆ

```markdown
# æ¨¡å‹è¯„ä¼°æŠ¥å‘Š

## åŸºç¡€æŒ‡æ ‡
- **æ€»ä½“MAE**: 3.85 px âœ… (ç›®æ ‡: <5px)
- **ç²¾åº¦@5px**: 98.7% âœ… (ç›®æ ‡: >98%)
- **ç²¾åº¦@3px**: 87.3% âœ… (ç›®æ ‡: >85%)
- **æ¨ç†é€Ÿåº¦**: 12.5ms (CPU) âœ…

## åˆ†é¡¹æŒ‡æ ‡
| ç»„ä»¶ | MAE | ç²¾åº¦@5px | ç²¾åº¦@3px |
|-----|-----|----------|----------|
| ç¼ºå£ | 3.92px | 98.5% | 86.8% |
| æ»‘å— | 3.78px | 98.9% | 87.8% |

## å›°éš¾æ ·æœ¬è¡¨ç°
| æ ·æœ¬ç±»å‹ | å æ¯” | MAE | ç²¾åº¦@5px |
|---------|-----|-----|----------|
| æ™®é€šæ ·æœ¬ | 70% | 3.2px | 99.5% |
| æ··æ·†ç¼ºå£ | 15% | 5.1px | 95.2% |
| æ—‹è½¬ç¼ºå£ | 10% | 4.8px | 96.8% |
| å™ªå£°æ ·æœ¬ | 5% | 6.2px | 92.1% |

## è®­ç»ƒè¿‡ç¨‹
- **æ€»è®­ç»ƒæ—¶é—´**: 8h 35min
- **æœ€ä½³Epoch**: 142/160
- **æœ€ç»ˆLoss**: 0.0234
- **è¿‡æ‹Ÿåˆç¨‹åº¦**: è½»å¾®ï¼ˆtrain/val losså·®å¼‚<10%ï¼‰
```

### 3. å¤±è´¥æ¡ˆä¾‹åˆ†æ

```python
# æ”¶é›†å¤±è´¥æ¡ˆä¾‹
def analyze_failures(predictions, targets, threshold=10):
    failures = []

    for i in range(len(predictions)):
        error = np.abs(predictions[i] - targets[i]).mean()
        if error > threshold:
            failures.append({
                'index': i,
                'error': error,
                'prediction': predictions[i],
                'target': targets[i],
                'image_path': image_paths[i]
            })

    # åˆ†æå¤±è´¥åŸå› 
    failure_reasons = {
        'extreme_rotation': 0,
        'heavy_noise': 0,
        'multiple_gaps': 0,
        'edge_position': 0,
        'low_contrast': 0
    }

    for failure in failures:
        # åˆ†æå…·ä½“åŸå› ...
        pass

    return failures, failure_reasons
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥æŒ‡å—

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

| é—®é¢˜          | å¯èƒ½åŸå›      | è§£å†³æ–¹æ¡ˆ                |
| ----------- | -------- | ------------------- |
| **Lossä¸ä¸‹é™** | å­¦ä¹ ç‡è¿‡å°/è¿‡å¤§ | è°ƒæ•´å­¦ä¹ ç‡ï¼Œæ£€æŸ¥æ•°æ®æ ‡ç­¾        |
| **è¿‡æ‹Ÿåˆä¸¥é‡**   | æ•°æ®å¢å¼ºä¸è¶³   | å¢åŠ æ•°æ®å¢å¼ºï¼Œä½¿ç”¨dropout    |
| **MAEçªç„¶å¢å¤§** | æ¢¯åº¦çˆ†ç‚¸     | å‡å°å­¦ä¹ ç‡ï¼Œå¢åŠ æ¢¯åº¦è£å‰ª        |
| **æ˜¾å­˜æº¢å‡º**    | æ‰¹æ¬¡è¿‡å¤§     | å‡å°batch_sizeï¼Œä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ |
| **è®­ç»ƒé€Ÿåº¦æ…¢**   | æ•°æ®åŠ è½½ç“¶é¢ˆ   | å¢åŠ num_workersï¼Œä½¿ç”¨ç¼“å­˜  |
| **ç²¾åº¦æå‡åœæ»**  | è¾¾åˆ°æ¨¡å‹å®¹é‡ä¸Šé™ | å¢åŠ æ¨¡å‹å¤æ‚åº¦ï¼Œæ”¹è¿›æ¶æ„        |

### è®­ç»ƒæ—¥å¿—ç›‘æ§

```bash
# å®æ—¶ç›‘æ§å…³é”®æŒ‡æ ‡
watch -n 1 'tail -20 logs/training.log | grep -E "Loss|MAE|Acc"'

# æ£€æŸ¥GPUä½¿ç”¨ç‡
nvidia-smi -l 1

# ç›‘æ§å†…å­˜ä½¿ç”¨
htop

# æ£€æŸ¥æ•°æ®åŠ è½½é€Ÿåº¦
python scripts/benchmark_dataloader.py
```

### è°ƒè¯•æŠ€å·§

1. **æ¢¯åº¦æ£€æŸ¥**
   
   ```python
   # æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
   for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        if grad_norm > 100:
            print(f"Warning: Large gradient in {name}: {grad_norm}")
   ```

2. **æ•°æ®éªŒè¯**
   
   ```python
   # éªŒè¯æ•°æ®æ ‡ç­¾
   def validate_dataset(dataset):
    errors = []
    for i, sample in enumerate(dataset):
        if not (0 <= sample['gap_x'] <= 512):
            errors.append(f"Sample {i}: Invalid gap_x")
        if not (0 <= sample['gap_y'] <= 256):
            errors.append(f"Sample {i}: Invalid gap_y")
    return errors
   ```

3. **æ¨¡å‹è¾“å‡ºæ£€æŸ¥**
   
   ```python
   # æ£€æŸ¥è¾“å‡ºèŒƒå›´
   outputs = model(batch)
   assert outputs['heatmap_gap'].min() >= 0
   assert outputs['heatmap_gap'].max() <= 1
   assert outputs['offset_gap'].abs().max() <= 0.5
   ```