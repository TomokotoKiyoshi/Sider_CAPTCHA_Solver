# Lite-HRNet-18+LiteFPN è®­ç»ƒç³»ç»Ÿæ¶æ„è®¾è®¡

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ](#ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ)
- [æ ¸å¿ƒæ¨¡å—è®¾è®¡](#æ ¸å¿ƒæ¨¡å—è®¾è®¡)
- [è®­ç»ƒè„šæœ¬å®ç°](#è®­ç»ƒè„šæœ¬å®ç°)
- [éªŒè¯è¯„ä¼°ç³»ç»Ÿ](#éªŒè¯è¯„ä¼°ç³»ç»Ÿ)
- [å¯è§†åŒ–ç›‘æ§ç³»ç»Ÿ](#å¯è§†åŒ–ç›‘æ§ç³»ç»Ÿ)
- [é…ç½®ç®¡ç†ç³»ç»Ÿ](#é…ç½®ç®¡ç†ç³»ç»Ÿ)
- [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     è®­ç»ƒç³»ç»Ÿä¸»æ§åˆ¶å™¨                           â”‚
â”‚                  scripts/training/train.py                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                        â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  é…ç½®ç®¡ç†å™¨   â”‚                      â”‚   æ•°æ®ç®¡é“ç³»ç»Ÿ     â”‚
    â”‚ ConfigManagerâ”‚                      â”‚ DataPipeline     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                        â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     è®­ç»ƒå¾ªç¯æ§åˆ¶å™¨                         â”‚
    â”‚                    TrainingEngine                        â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
    â”‚  â”‚ Optimizerâ”‚  â”‚Scheduler â”‚  â”‚   EMA    â”‚  â”‚  AMP    â”‚â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   éªŒè¯è¯„ä¼°ç³»ç»Ÿ      â”‚              â”‚   å¯è§†åŒ–ç›‘æ§ç³»ç»Ÿ     â”‚
    â”‚   Validator       â”‚              â”‚   Visualizer        â”‚
    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚ â”‚ Metrics      â”‚ â”‚              â”‚ â”‚  TensorBoard   â”‚ â”‚
    â”‚ â”‚ EarlyStoppingâ”‚ â”‚              â”‚ â”‚  Logging       â”‚ â”‚
    â”‚ â”‚ Checkpointingâ”‚ â”‚              â”‚ â”‚  Profiling     â”‚ â”‚
    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **æ¨¡å—åŒ–è®¾è®¡**: æ¯ä¸ªåŠŸèƒ½ç‹¬ç«‹æˆæ¨¡å—ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
2. **é…ç½®é©±åŠ¨**: æ‰€æœ‰å‚æ•°é€šè¿‡YAMLé…ç½®æ–‡ä»¶ç®¡ç†
3. **å®¹é”™æ€§**: æ”¯æŒè®­ç»ƒä¸­æ–­æ¢å¤å’Œå¼‚å¸¸å¤„ç†
4. **å¯æ‰©å±•æ€§**: æ˜“äºæ·»åŠ æ–°çš„åº¦é‡æŒ‡æ ‡å’Œå¯è§†åŒ–åŠŸèƒ½
5. **æ€§èƒ½ä¼˜åŒ–**: æ”¯æŒæ··åˆç²¾åº¦ã€æ•°æ®å¹¶è¡Œå’Œå†…å­˜ä¼˜åŒ–

---

## ğŸ”§ æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 1. é…ç½®ç®¡ç†å™¨ (ConfigManager)

```python
# scripts/training/config_manager.py
import yaml
from pathlib import Path
from typing import Dict, Any
import torch

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨ - è´Ÿè´£åŠ è½½ã€éªŒè¯å’Œç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°"""
    
    def __init__(self, config_path: str):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self._validate_config()
        self._setup_paths()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _validate_config(self):
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        required_keys = ['model', 'optimizer', 'sched', 'train', 'eval']
        for key in required_keys:
            assert key in self.config, f"Missing required config: {key}"
    
    def _setup_paths(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        paths = [
            Path(self.config['checkpoints']['save_dir']),
            Path(self.config['logging']['log_dir']),
            Path(self.config['logging']['tensorboard_dir'])
        ]
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)
    
    def get_device(self) -> torch.device:
        """è·å–è®­ç»ƒè®¾å¤‡"""
        device_str = self.config['hardware']['device']
        if device_str == 'cuda' and torch.cuda.is_available():
            return torch.device('cuda')
        return torch.device('cpu')
```

### 2. æ•°æ®ç®¡é“ç³»ç»Ÿ (DataPipeline)

```python
# scripts/training/data_pipeline.py
import torch
from torch.utils.data import DataLoader, Dataset
from typing import Tuple
import numpy as np
from pathlib import Path

class CaptchaDataset(Dataset):
    """æ»‘å—éªŒè¯ç æ•°æ®é›†"""
    
    def __init__(self, data_dir: str, mode: str = 'train'):
        self.data_dir = Path(data_dir)
        self.mode = mode
        self.samples = self._load_samples()
        
    def _load_samples(self):
        """åŠ è½½æ•°æ®æ ·æœ¬"""
        # å®ç°æ•°æ®åŠ è½½é€»è¾‘
        pass
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        """è¿”å›å•ä¸ªæ ·æœ¬"""
        sample = self.samples[idx]
        # è¿”å›æ ¼å¼ï¼š
        # {
        #     'image': tensor,         # [4, 256, 512]
        #     'gap_coords': tensor,    # [2]
        #     'slider_coords': tensor, # [2]
        #     'has_rotation': bool,
        #     'has_noise': bool
        # }
        return sample

class DataPipeline:
    """æ•°æ®ç®¡é“ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.train_loader = None
        self.val_loader = None
        
    def setup(self):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨"""
        train_dataset = CaptchaDataset('data/train', mode='train')
        val_dataset = CaptchaDataset('data/val', mode='val')
        
        # è®­ç»ƒæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['train']['batch_size'],
            shuffle=True,
            num_workers=self.config['train']['num_workers'],
            pin_memory=self.config['train']['pin_memory'],
            persistent_workers=True,
            prefetch_factor=2
        )
        
        # éªŒè¯æ•°æ®åŠ è½½å™¨
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['train']['batch_size'],
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
```

### 3. è®­ç»ƒå¼•æ“ (TrainingEngine)

```python
# scripts/training/training_engine.py
import torch
import torch.nn as nn
from torch.cuda.amp import GradScaler, autocast
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from typing import Dict, Any
import time

class TrainingEngine:
    """è®­ç»ƒå¼•æ“ - æ ¸å¿ƒè®­ç»ƒå¾ªç¯ç®¡ç†"""
    
    def __init__(self, model: nn.Module, config: Dict, device: torch.device):
        self.model = model.to(device)
        self.config = config
        self.device = device
        
        # ä¼˜åŒ–å™¨è®¾ç½®
        self.optimizer = self._setup_optimizer()
        self.scheduler = self._setup_scheduler()
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = config['train']['amp'] == 'bf16'
        self.scaler = GradScaler() if self.use_amp else None
        
        # EMAè®¾ç½®
        self.ema = self._setup_ema() if config['optimizer']['ema_decay'] else None
        
        # å†…å­˜å¸ƒå±€ä¼˜åŒ–
        if config['train']['channels_last']:
            self.model = self.model.to(memory_format=torch.channels_last)
    
    def _setup_optimizer(self) -> torch.optim.Optimizer:
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        opt_cfg = self.config['optimizer']
        return AdamW(
            self.model.parameters(),
            lr=opt_cfg['lr'],
            betas=opt_cfg['betas'],
            eps=opt_cfg['eps'],
            weight_decay=opt_cfg['weight_decay']
        )
    
    def _setup_scheduler(self):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        sched_cfg = self.config['sched']
        return CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=sched_cfg['warmup_epochs'],
            T_mult=2,
            eta_min=sched_cfg['cosine_min_lr']
        )
    
    def _setup_ema(self):
        """è®¾ç½®æŒ‡æ•°ç§»åŠ¨å¹³å‡"""
        from copy import deepcopy
        ema_model = deepcopy(self.model)
        ema_model.eval()
        return ema_model
    
    def train_epoch(self, dataloader, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        metrics = {'loss': 0, 'gap_mae': 0, 'slider_mae': 0}
        
        for batch_idx, batch in enumerate(dataloader):
            # æ•°æ®ä¼ è¾“åˆ°è®¾å¤‡
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # å‰å‘ä¼ æ’­ (å¸¦æ··åˆç²¾åº¦)
            with autocast(enabled=self.use_amp, dtype=torch.bfloat16):
                outputs = self.model(batch['image'])
                loss = self._compute_loss(outputs, batch)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            if self.use_amp:
                self.scaler.scale(loss).backward()
                # æ¢¯åº¦è£å‰ª
                self.scaler.unscale_(self.optimizer)
                nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config['optimizer']['clip_grad_norm']
                )
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['optimizer']['clip_grad_norm']
                )
                self.optimizer.step()
            
            # æ›´æ–°EMA
            if self.ema:
                self._update_ema()
            
            # è®°å½•æŒ‡æ ‡
            metrics['loss'] += loss.item()
            
        # å¹³å‡æŒ‡æ ‡
        num_batches = len(dataloader)
        metrics = {k: v / num_batches for k, v in metrics.items()}
        
        return metrics
    
    def _compute_loss(self, outputs: Dict, targets: Dict) -> torch.Tensor:
        """è®¡ç®—æŸå¤±å‡½æ•°"""
        # å®ç°CenterNetæŸå¤±
        pass
    
    def _update_ema(self):
        """æ›´æ–°EMAæ¨¡å‹"""
        decay = self.config['optimizer']['ema_decay']
        with torch.no_grad():
            for ema_p, model_p in zip(self.ema.parameters(), self.model.parameters()):
                ema_p.data.mul_(decay).add_(model_p.data, alpha=1-decay)
```

### 4. éªŒè¯è¯„ä¼°ç³»ç»Ÿ (Validator)

```python
# scripts/training/validator.py
import torch
import torch.nn as nn
from typing import Dict, List
import numpy as np

class Validator:
    """éªŒè¯è¯„ä¼°ç³»ç»Ÿ"""
    
    def __init__(self, config: Dict, device: torch.device):
        self.config = config
        self.device = device
        self.best_metric = -float('inf')
        self.patience_counter = 0
        self.metrics_history = []
        
        # ç¬¬äºŒé“é˜²æŠ¤æŒ‡æ ‡
        if 'second_guard' in config['eval']['early_stopping']:
            guard_cfg = config['eval']['early_stopping']['second_guard']
            self.second_guard_metric = guard_cfg['metric']
            self.second_guard_mode = guard_cfg['mode']
            self.second_guard_min_delta = guard_cfg['min_delta']
            self.second_guard_best = float('inf') if guard_cfg['mode'] == 'min' else -float('inf')
        else:
            self.second_guard_metric = None
        
    def validate(self, model: nn.Module, dataloader, epoch: int) -> Dict[str, float]:
        """æ‰§è¡ŒéªŒè¯"""
        model.eval()
        
        # åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†å™¨
        mae_gap_list = []
        mae_slider_list = []
        hit_le_1px = 0
        hit_le_2px = 0
        hit_le_5px = 0
        total_samples = 0
        
        with torch.no_grad():
            for batch in dataloader:
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # å‰å‘ä¼ æ’­
                outputs = model(batch['image'])
                predictions = model.decode_predictions(outputs)
                
                # è®¡ç®—è¯¯å·®
                gap_error = torch.abs(
                    predictions['gap_coords'] - batch['gap_coords']
                ).mean(dim=1)  # [B]
                
                slider_error = torch.abs(
                    predictions['slider_coords'] - batch['slider_coords']
                ).mean(dim=1)  # [B]
                
                # æ”¶é›†MAE
                mae_gap_list.extend(gap_error.cpu().numpy())
                mae_slider_list.extend(slider_error.cpu().numpy())
                
                # è®¡ç®—å‘½ä¸­ç‡
                total_error = (gap_error + slider_error) / 2
                hit_le_1px += (total_error <= 1).sum().item()
                hit_le_2px += (total_error <= 2).sum().item()
                hit_le_5px += (total_error <= 5).sum().item()
                total_samples += batch['image'].size(0)
        
        # æ±‡æ€»æŒ‡æ ‡
        metrics = {
            'mae_px': np.mean(mae_gap_list + mae_slider_list),
            'rmse_px': np.sqrt(np.mean(np.square(mae_gap_list + mae_slider_list))),
            'hit_le_1px': hit_le_1px / total_samples * 100,
            'hit_le_2px': hit_le_2px / total_samples * 100,
            'hit_le_5px': hit_le_5px / total_samples * 100,
            'gap_mae': np.mean(mae_gap_list),
            'slider_mae': np.mean(mae_slider_list)
        }
        
        # ä¿å­˜æŒ‡æ ‡å†å²
        self.metrics_history.append(metrics)
        
        # æ—©åœæ£€æŸ¥
        select_metric = metrics[self.config['eval']['select_by']]
        if self._check_early_stopping(select_metric, epoch):
            metrics['early_stop'] = True
        
        return metrics
    
    def _check_early_stopping(self, metric: float, epoch: int) -> bool:
        """æ£€æŸ¥æ—©åœæ¡ä»¶"""
        cfg = self.config['eval']['early_stopping']
        
        # æœªè¾¾åˆ°æœ€å°è®­ç»ƒè½®æ•°
        if epoch < cfg['min_epochs']:
            return False
        
        # ä¸»æŒ‡æ ‡æ£€æŸ¥ï¼ˆhit_le_5pxï¼‰
        has_primary_improvement = False
        if metric > self.best_metric:
            self.best_metric = metric
            self.patience_counter = 0
            has_primary_improvement = True
        else:
            self.patience_counter += 1
        
        # ç¬¬äºŒé“é˜²æŠ¤æ£€æŸ¥ï¼ˆmae_pxï¼‰
        if self.second_guard_metric and epoch >= cfg['min_epochs']:
            current_guard_metric = self.metrics_history[-1][self.second_guard_metric]
            
            if self.second_guard_mode == 'min':
                # å¯¹äºmae_pxï¼Œè¶Šå°è¶Šå¥½
                improvement = self.second_guard_best - current_guard_metric
                if improvement > self.second_guard_min_delta:
                    self.second_guard_best = current_guard_metric
                    # ç¬¬äºŒæŒ‡æ ‡æœ‰æ˜¾è‘—æ”¹å–„ï¼Œé‡ç½®è€å¿ƒè®¡æ•°
                    self.patience_counter = max(0, self.patience_counter - 3)
                    print(f"Second guard: {self.second_guard_metric} improved by {improvement:.3f}px")
            else:
                # å¯¹äºå…¶ä»–æŒ‡æ ‡ï¼Œè¶Šå¤§è¶Šå¥½
                improvement = current_guard_metric - self.second_guard_best
                if improvement > self.second_guard_min_delta:
                    self.second_guard_best = current_guard_metric
                    self.patience_counter = max(0, self.patience_counter - 3)
                    print(f"Second guard: {self.second_guard_metric} improved by {improvement:.3f}")
        
        # æ£€æŸ¥è€å¿ƒå€¼
        if self.patience_counter >= cfg['patience']:
            print(f"Early stopping triggered at epoch {epoch}")
            print(f"Best hit@5px: {self.best_metric:.2f}%")
            if self.second_guard_metric:
                print(f"Best {self.second_guard_metric}: {self.second_guard_best:.3f}")
            return True
        
        return False
```

### 5. å¯è§†åŒ–ç›‘æ§ç³»ç»Ÿ (Visualizer)

```python
# scripts/training/visualizer.py
import torch
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import cv2
from typing import Dict, Any
from pathlib import Path

class Visualizer:
    """å¯è§†åŒ–ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.writer = SummaryWriter(config['logging']['tensorboard_dir'])
        self.log_dir = Path(config['logging']['log_dir'])
        
    def log_scalars(self, metrics: Dict[str, float], step: int, prefix: str = ''):
        """è®°å½•æ ‡é‡æŒ‡æ ‡"""
        for key, value in metrics.items():
            self.writer.add_scalar(f'{prefix}/{key}', value, step)
    
    def log_learning_rate(self, lr: float, step: int):
        """è®°å½•å­¦ä¹ ç‡"""
        self.writer.add_scalar('lr/current', lr, step)
    
    def log_histograms(self, model: nn.Module, step: int):
        """è®°å½•æƒé‡å’Œæ¢¯åº¦ç›´æ–¹å›¾"""
        for name, param in model.named_parameters():
            if param.requires_grad and param.grad is not None:
                # æƒé‡ç›´æ–¹å›¾
                self.writer.add_histogram(
                    f'hist/weight_{name}', 
                    param.data, 
                    step
                )
                # æ¢¯åº¦ç›´æ–¹å›¾
                self.writer.add_histogram(
                    f'hist/grad_{name}', 
                    param.grad, 
                    step
                )
    
    def log_predictions(self, images, predictions, targets, step: int):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        # é€‰æ‹©å‰Nä¸ªæ ·æœ¬
        num_vis = min(4, images.size(0))
        
        for i in range(num_vis):
            # ç»˜åˆ¶é¢„æµ‹å’ŒçœŸå®ä½ç½®
            img = images[i].cpu().numpy().transpose(1, 2, 0)
            img = (img * 255).astype(np.uint8)[:, :, :3]  # åªå–RGB
            
            # çœŸå®ä½ç½®ï¼ˆç»¿è‰²ï¼‰
            gt_gap = targets['gap_coords'][i].cpu().numpy()
            cv2.circle(img, tuple(gt_gap.astype(int)), 5, (0, 255, 0), -1)
            
            # é¢„æµ‹ä½ç½®ï¼ˆçº¢è‰²ï¼‰
            pred_gap = predictions['gap_coords'][i].cpu().numpy()
            cv2.circle(img, tuple(pred_gap.astype(int)), 5, (255, 0, 0), -1)
            
            # æ·»åŠ åˆ°TensorBoard
            self.writer.add_image(
                f'vis/overlay_gap_{i}', 
                img.transpose(2, 0, 1), 
                step
            )
    
    def log_failure_cases(self, failures: List[Dict], step: int):
        """è®°å½•å¤±è´¥æ¡ˆä¾‹"""
        for i, failure in enumerate(failures[:self.config['eval']['vis_fail_k']]):
            # å¯è§†åŒ–å¤±è´¥æ¡ˆä¾‹
            self.writer.add_text(
                f'failures/case_{i}',
                f"Error: {failure['error']:.2f}px",
                step
            )
    
    def close(self):
        """å…³é—­writer"""
        self.writer.close()
```

---

## ğŸ“ è®­ç»ƒè„šæœ¬å®ç°

### ä¸»è®­ç»ƒè„šæœ¬

```python
# scripts/training/train.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Lite-HRNet-18+LiteFPN è®­ç»ƒä¸»è„šæœ¬
"""
import argparse
import torch
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.models import create_lite_hrnet_18_fpn
from config_manager import ConfigManager
from data_pipeline import DataPipeline
from training_engine import TrainingEngine
from validator import Validator
from visualizer import Visualizer

def main():
    # å‚æ•°è§£æ
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, 
                       default='config/training_config.yaml')
    parser.add_argument('--resume', type=str, default=None)
    args = parser.parse_args()
    
    # åˆå§‹åŒ–é…ç½®
    config_manager = ConfigManager(args.config)
    config = config_manager.config
    device = config_manager.get_device()
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = create_lite_hrnet_18_fpn(config['model'])
    
    # åˆå§‹åŒ–æ•°æ®ç®¡é“
    data_pipeline = DataPipeline(config)
    data_pipeline.setup()
    
    # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
    engine = TrainingEngine(model, config, device)
    validator = Validator(config, device)
    visualizer = Visualizer(config)
    
    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
    start_epoch = 0
    if args.resume:
        checkpoint = torch.load(args.resume)
        model.load_state_dict(checkpoint['model_state_dict'])
        engine.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        print(f"Resumed from epoch {start_epoch}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, config['sched']['epochs']):
        print(f"\n{'='*50}")
        print(f"Epoch {epoch+1}/{config['sched']['epochs']}")
        print(f"{'='*50}")
        
        # è®­ç»ƒé˜¶æ®µ
        train_metrics = engine.train_epoch(
            data_pipeline.train_loader, 
            epoch
        )
        
        # éªŒè¯é˜¶æ®µ
        val_metrics = validator.validate(
            model, 
            data_pipeline.val_loader, 
            epoch
        )
        
        # è®°å½•åˆ°TensorBoard
        visualizer.log_scalars(train_metrics, epoch, 'train')
        visualizer.log_scalars(val_metrics, epoch, 'val')
        visualizer.log_learning_rate(
            engine.scheduler.get_last_lr()[0], 
            epoch
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        engine.scheduler.step()
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % config['checkpoints']['save_interval'] == 0:
            save_checkpoint(model, engine, epoch, config)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics[config['eval']['select_by']] == validator.best_metric:
            save_best_model(model, config)
        
        # æ—©åœæ£€æŸ¥
        if val_metrics.get('early_stop', False):
            print("Early stopping triggered!")
            break
        
        # æ‰“å°è¿›åº¦
        print(f"Train Loss: {train_metrics['loss']:.4f}")
        print(f"Val MAE: {val_metrics['mae_px']:.2f}px")
        print(f"Val hit@5px: {val_metrics['hit_le_5px']:.2f}%")
    
    visualizer.close()
    print("Training completed!")

def save_checkpoint(model, engine, epoch, config):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': engine.optimizer.state_dict(),
        'scheduler_state_dict': engine.scheduler.state_dict(),
    }
    
    # ä¿å­˜epochæ£€æŸ¥ç‚¹
    path = Path(config['checkpoints']['save_dir']) / f"epoch_{epoch:03d}.pth"
    torch.save(checkpoint, path)
    
    # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
    latest_path = Path(config['checkpoints']['save_dir']) / "last.pth"
    torch.save(checkpoint, latest_path)

def save_best_model(model, config):
    """ä¿å­˜æœ€ä½³æ¨¡å‹"""
    path = Path(config['checkpoints']['save_dir']) / "best_model.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config
    }, path)
    print(f"Saved best model to {path}")

if __name__ == "__main__":
    main()
```

---

## ğŸš€ å¯åŠ¨å‘½ä»¤

### åŸºç¡€è®­ç»ƒ
```bash
python scripts/training/train.py --config config/training_config.yaml
```

### æ¢å¤è®­ç»ƒ
```bash
python scripts/training/train.py \
    --config config/training_config.yaml \
    --resume checkpoints/1.1.0/last.pth
```

### ç›‘æ§è®­ç»ƒ
```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir logs/tensorboard/1.1.0 --port 6006

# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f logs/log-1.1.0/training.log
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. æ•°æ®åŠ è½½ä¼˜åŒ–
- **é¢„å–å› å­**: `prefetch_factor=2`
- **æŒä¹…åŒ–workers**: `persistent_workers=True`
- **å›ºå®šå†…å­˜**: `pin_memory=True`
- **å¹¶è¡Œworkers**: `num_workers=24`

### 2. è®­ç»ƒä¼˜åŒ–
- **æ··åˆç²¾åº¦**: BFloat16è‡ªåŠ¨æ··åˆç²¾åº¦
- **æ¢¯åº¦ç´¯ç§¯**: æ”¯æŒå°æ˜¾å­˜å¤§batchè®­ç»ƒ
- **å†…å­˜å¸ƒå±€**: Channels-lastä¼˜åŒ–
- **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

### 3. æ¨¡å‹ä¼˜åŒ–
- **EMA**: æŒ‡æ•°ç§»åŠ¨å¹³å‡æå‡ç¨³å®šæ€§
- **æ—©åœæœºåˆ¶**: é˜²æ­¢è¿‡æ‹Ÿåˆ
- **å­¦ä¹ ç‡è°ƒåº¦**: Cosineé€€ç«with warm restarts

---

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡

### æ ¸å¿ƒæŒ‡æ ‡
- **MAE**: å¹³å‡ç»å¯¹è¯¯å·® < 2px
- **RMSE**: å‡æ–¹æ ¹è¯¯å·® < 3px
- **hit@1px**: 1åƒç´ å†…å‘½ä¸­ç‡ > 90%
- **hit@2px**: 2åƒç´ å†…å‘½ä¸­ç‡ > 95%
- **hit@5px**: 5åƒç´ å†…å‘½ä¸­ç‡ > 99%ï¼ˆé€‰æ‹©æŒ‡æ ‡ï¼‰

### TensorBoardç›‘æ§é¡¹
- æŸå¤±æ›²çº¿ï¼š`loss/train`, `loss/val`
- è¯„ä¼°æŒ‡æ ‡ï¼š`metrics/mae_px`, `metrics/hit_le_5px`
- å­¦ä¹ ç‡ï¼š`lr/current`
- å¯è§†åŒ–ï¼š`vis/overlay_gap`, `vis/overlay_slider`
- ç›´æ–¹å›¾ï¼š`hist/grad_*`, `hist/weight_*`

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|-----|------|---------|
| OOMé”™è¯¯ | Batch sizeè¿‡å¤§ | å‡å°batch_sizeæˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ |
| Lossä¸ä¸‹é™ | å­¦ä¹ ç‡ä¸åˆé€‚ | è°ƒæ•´å­¦ä¹ ç‡æˆ–æ£€æŸ¥æ•°æ® |
| éªŒè¯æŒ‡æ ‡éœ‡è¡ | è¿‡æ‹Ÿåˆ | å¢åŠ æ•°æ®å¢å¼ºæˆ–æ­£åˆ™åŒ– |
| è®­ç»ƒé€Ÿåº¦æ…¢ | æ•°æ®åŠ è½½ç“¶é¢ˆ | å¢åŠ num_workers |

### è°ƒè¯•å‘½ä»¤
```bash
# GPUç›‘æ§
watch -n 1 nvidia-smi

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
grep -E "epoch|loss|mae" logs/log-1.1.0/training.log | tail -20

# æ£€æŸ¥checkpoint
python -c "import torch; print(torch.load('checkpoints/1.1.0/last.pth').keys())"
```