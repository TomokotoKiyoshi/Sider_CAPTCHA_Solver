#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CAPTCHAæ•°æ®é›†é¢„å¤„ç†è„šæœ¬ - æµå¼ç‰ˆæœ¬
ä½¿ç”¨æµå¼å†™å…¥ï¼Œå½»åº•è§£å†³å†…å­˜é—®é¢˜
"""
import sys
from pathlib import Path
import argparse
from datetime import datetime
import psutil
import gc

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parents[2]  # ç°åœ¨éœ€è¦å‘ä¸Šä¸¤å±‚åˆ°è¾¾é¡¹ç›®æ ¹ç›®å½•
sys.path.insert(0, str(project_root))

# å¯¼å…¥æµå¼æ•°æ®é›†ç”Ÿæˆå™¨å’Œæ•°æ®é›†åˆ’åˆ†å™¨
from src.preprocessing.dataset_generator import StreamingDatasetGenerator
from src.preprocessing.dataset_splitter import DatasetSplitter


def monitor_memory():
    """ç›‘æ§å†…å­˜ä½¿ç”¨"""
    process = psutil.Process()
    mem_info = process.memory_info()
    mem_mb = mem_info.rss / 1024 / 1024
    mem_percent = process.memory_percent()
    return mem_mb, mem_percent


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='CAPTCHA Dataset Preprocessing (Streaming Version)')
    parser.add_argument('--split', type=str, default='train',
                       choices=['train', 'val', 'test', 'all'],
                       help='Dataset split to generate')
    parser.add_argument('--max-samples', type=int, default=None,
                       help='Maximum number of samples to process (for testing)')
    parser.add_argument('--config', type=str, default=None,
                       help='Path to preprocessing config file')
    parser.add_argument('--num-workers', type=int, default=None,
                       help='Number of worker processes')
    parser.add_argument('--batch-size', type=int, default=None,
                       help='Batch size for saving')
    parser.add_argument('--auto-split', action='store_true',
                       help='Automatically split dataset by pic_id')
    parser.add_argument('--train-ratio', type=float, default=None,
                       help='Training set ratio (overrides config file)')
    parser.add_argument('--val-ratio', type=float, default=None,
                       help='Validation set ratio (overrides config file)')
    parser.add_argument('--test-ratio', type=float, default=None,
                       help='Test set ratio (overrides config file)')
    parser.add_argument('--seed', type=int, default=None,
                       help='Random seed for split (overrides config file)')
    parser.add_argument('--shuffle', type=lambda x: x.lower() in ['true', '1', 'yes', 'on'],
                       default=None,
                       help='Whether to shuffle data before splitting (overrides config file, true/false)')
    parser.add_argument('--no-shuffle', action='store_true',
                       help='Disable shuffling (equivalent to --shuffle false)')
    
    args = parser.parse_args()
    
    # å¤„ç†no-shuffleå‚æ•°
    if args.no_shuffle:
        args.shuffle = False
    
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    print("=" * 80)
    print("CAPTCHA Dataset Preprocessing (Streaming Version)")
    print("=" * 80)
    print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ˜¾ç¤ºå†…å­˜ä¿¡æ¯
    mem_mb, mem_percent = monitor_memory()
    print(f"Initial memory: {mem_mb:.2f} MB ({mem_percent:.1f}%)")
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    if args.config is None:
        config_path = project_root / "config" / "preprocessing_config.yaml"
    else:
        config_path = Path(args.config)
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    import yaml
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–è·¯å¾„ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„ï¼‰
    def resolve_path(path_str):
        path = Path(path_str)
        if not path.is_absolute():
            path = project_root / path
        return path
    
    data_root = resolve_path(config['paths']['data_root'])
    output_dir = resolve_path(config['paths']['output_dir'])
    labels_file = resolve_path(config['paths']['labels_file'])
    
    print(f"\nConfiguration:")
    print(f"  Config file: {config_path}")
    print(f"  Data root: {data_root}")
    print(f"  Output dir: {output_dir}")
    print(f"  Labels file: {labels_file}")
    print(f"  Split: {args.split}")
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–æ•°æ®é›†åˆ’åˆ†å‚æ•°
    data_split_config = config.get('data_split', {})
    split_ratios = data_split_config.get('split_ratio', {})
    split_options = data_split_config.get('options', {})
    
    # å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶
    train_ratio = args.train_ratio if args.train_ratio is not None else split_ratios.get('train', 0.8)
    val_ratio = args.val_ratio if args.val_ratio is not None else split_ratios.get('val', 0.1)
    test_ratio = args.test_ratio if args.test_ratio is not None else split_ratios.get('test', 0.1)
    seed = args.seed if args.seed is not None else split_options.get('random_seed', 42)
    
    # åˆ¤æ–­æ˜¯å¦å¯ç”¨è‡ªåŠ¨åˆ’åˆ†ï¼ˆå‘½ä»¤è¡Œå‚æ•° > é…ç½®æ–‡ä»¶ï¼‰
    # æ³¨æ„ï¼šå¦‚æœå‘½ä»¤è¡Œæœ‰--auto-splitï¼Œåˆ™ä¸ºTrueï¼›å¦åˆ™è¯»å–é…ç½®æ–‡ä»¶
    auto_split_enabled = args.auto_split or split_options.get('auto_split', False)
    
    # æ˜¾ç¤ºé…ç½®æ¥æºä¿¡æ¯
    print(f"\nğŸ“‹ Configuration Source:")
    print(f"  Config file: {config_path}")
    print(f"  Auto-split setting in config: {split_options.get('auto_split', False)}")
    print(f"  Command line --auto-split: {args.auto_split}")
    print(f"  Final auto_split_enabled: {auto_split_enabled}")
    
    # æ£€æŸ¥æ ‡ç­¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not labels_file.exists():
        print(f"\nâŒ Error: Labels file not found: {labels_file}")
        print("Please run the captcha generation script first.")
        return 1
    
    print(f"\nOptimizations:")
    print(f"âœ… Streaming write with reusable buffers")
    print(f"âœ… No intermediate lists or accumulation")
    print(f"âœ… Separate file saves (no zip overhead)")
    print(f"âœ… Memory usage is constant and predictable")
    if auto_split_enabled:
        print(f"âœ… Auto-split by pic_id enabled (train:{train_ratio}, val:{val_ratio}, test:{test_ratio}, seed:{seed})")
        print(f"   Source: {'command line --auto-split' if args.auto_split else f'config file (auto_split: true)'}")
        print(f"   Will generate: train/val/test datasets")
    else:
        print(f"âš ï¸  Auto-split disabled - will only generate '{args.split}' dataset")
        print(f"   To enable auto-split: set auto_split: true in config or use --auto-split")
    print("=" * 80)
    
    try:
        # åˆ›å»ºæµå¼ç”Ÿæˆå™¨
        generator = StreamingDatasetGenerator(
            data_root=str(data_root),
            output_dir=str(output_dir),
            config_path=str(config_path),
            batch_size=args.batch_size,
            num_workers=args.num_workers
        )
        
        # å¦‚æœå¯ç”¨è‡ªåŠ¨åˆ’åˆ†
        if auto_split_enabled:
            print("\nğŸ”„ Auto-splitting dataset by pic_id...")
            
            # åŠ è½½æ‰€æœ‰æ ‡ç­¾
            import json
            with open(labels_file, 'r', encoding='utf-8') as f:
                all_labels = json.load(f)
            
            # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ ·æœ¬æ•°ï¼Œå…ˆæˆªå–
            if args.max_samples:
                all_labels = all_labels[:args.max_samples]
                print(f"Limited to {args.max_samples} samples for testing")
            
            # è·å–shuffleé…ç½®ï¼ˆå‘½ä»¤è¡Œå‚æ•° > é…ç½®æ–‡ä»¶ï¼‰
            shuffle = getattr(args, 'shuffle', None)
            if shuffle is None:
                shuffle = split_options.get('shuffle', True)
            
            # åˆ›å»ºæ•°æ®é›†åˆ’åˆ†å™¨ï¼ˆä½¿ç”¨ä»é…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œè·å–çš„å‚æ•°ï¼‰
            splitter = DatasetSplitter(
                train_ratio=train_ratio,
                val_ratio=val_ratio,
                test_ratio=test_ratio,
                seed=seed,
                shuffle=shuffle
            )
            
            # æ‰§è¡Œåˆ’åˆ†
            split_result = splitter.split_by_pic_id(all_labels)
            
            # ä¿å­˜åˆ’åˆ†ä¿¡æ¯ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„ï¼‰
            split_info_dir = config.get('output_structure', {}).get('split_info_subdir', 'split_info')
            splitter.save_split_info(split_result, str(output_dir / split_info_dir))
            
            # å¾—åˆ° split_result åï¼Œå°½å¿«é‡Šæ”¾å¤§è¡¨
            del all_labels
            gc.collect()
            
            # ä¿è¯å›ºå®šæ¬¡åºï¼Œä¸”æ¯æ¬¡å¤„ç†åä» split_result å¼¹å‡ºé‡Šæ”¾å¼•ç”¨
            for split_name in ("train", "val", "test"):
                split_labels = split_result.pop(split_name, [])
                if not split_labels:
                    continue  # è·³è¿‡ç©ºçš„split
                    
                print(f"\nğŸ“ Processing {split_name} split ({len(split_labels)} samples)...")
                mem_before_mb, mem_before_percent = monitor_memory()
                print(f"Memory before: {mem_before_mb:.2f} MB ({mem_before_percent:.1f}%)")
                
                # ç”Ÿæˆæ•°æ®é›†ï¼ˆä¼ å…¥é¢„åˆ’åˆ†çš„æ ‡ç­¾ï¼‰
                generator.generate_dataset(
                    str(labels_file),
                    split=split_name,
                    labels_subset=split_labels
                )
                
                # é‡Šæ”¾å½“å‰ split çš„åˆ—è¡¨
                del split_labels
                gc.collect()
                
                mem_after_mb, mem_after_percent = monitor_memory()
                print(f"Memory after: {mem_after_mb:.2f} MB ({mem_after_percent:.1f}%)")
                print(f"Memory delta: {mem_after_mb - mem_before_mb:+.2f} MB")
            
            # æœ€åæ¸…ç©ºå­—å…¸ï¼ŒäºŒæ¬¡å›æ”¶
            split_result.clear()
            gc.collect()
        else:
            # åŸå§‹é€»è¾‘ï¼šä¸è‡ªåŠ¨åˆ’åˆ†
            if args.split == 'all':
                splits = ['train', 'val', 'test']
            else:
                splits = [args.split]
            
            for split in splits:
                print(f"\nğŸ“ Processing {split} split...")
                
                # æ˜¾ç¤ºå¤„ç†å‰å†…å­˜
                mem_before_mb, mem_before_percent = monitor_memory()
                print(f"Memory before: {mem_before_mb:.2f} MB ({mem_before_percent:.1f}%)")
                
                # ç”Ÿæˆæ•°æ®é›†
                generator.generate_dataset(
                    str(labels_file),
                    split=split,
                    max_samples=args.max_samples
                )
                
                # æ˜¾ç¤ºå¤„ç†åå†…å­˜
                mem_after_mb, mem_after_percent = monitor_memory()
                print(f"Memory after: {mem_after_mb:.2f} MB ({mem_after_percent:.1f}%)")
                print(f"Memory delta: {mem_after_mb - mem_before_mb:+.2f} MB")
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect(2)
        
        # æœ€ç»ˆå†…å­˜ç»Ÿè®¡
        final_mem_mb, final_mem_percent = monitor_memory()
        print(f"\n" + "=" * 80)
        print(f"âœ… All processing completed successfully!")
        print(f"Final memory: {final_mem_mb:.2f} MB ({final_mem_percent:.1f}%)")
        print(f"Total memory growth: {final_mem_mb - mem_mb:+.2f} MB")
        print(f"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ Error during preprocessing: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())