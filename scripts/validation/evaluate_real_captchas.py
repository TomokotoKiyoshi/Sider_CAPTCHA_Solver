"""
çœŸå®éªŒè¯ç æ•°æ®é›†æ¨¡å‹è¯„ä¼°è„šæœ¬
è¯„ä¼°ä¸åŒepochæ¨¡å‹åœ¨çœŸå®æ•°æ®ä¸Šçš„è¡¨ç°
"""

import os
import sys
import json
import re
from pathlib import Path
from typing import Dict, List
import numpy as np
from tqdm import tqdm
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from sider_captcha_solver.predictor import CaptchaPredictor


class RealCaptchaEvaluator:
    """çœŸå®éªŒè¯ç è¯„ä¼°å™¨"""
    
    def __init__(self, 
                 data_dir: str = "data/real_captchas/annotated",
                 checkpoint_dir: str = "src/checkpoints/1.1.0",
                 device: str = "auto"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            data_dir: çœŸå®éªŒè¯ç æ•°æ®ç›®å½•
            checkpoint_dir: æ¨¡å‹checkpointç›®å½•
            device: è¿è¡Œè®¾å¤‡
        """
        self.data_dir = Path(data_dir)
        self.checkpoint_dir = Path(checkpoint_dir)
        self.device = device
        
        # æ£€æŸ¥ç›®å½•
        if not self.data_dir.exists():
            raise ValueError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
        if not self.checkpoint_dir.exists():
            raise ValueError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {self.checkpoint_dir}")
        
        # åŠ è½½æ•°æ®é›†
        self.image_files = self._load_dataset()
        print(f"âœ… Loaded {len(self.image_files)} real CAPTCHA images")
        
        # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹
        self.model_files = self._find_models()
        print(f"âœ… Found {len(self.model_files)} model files")
    
    def _load_dataset(self) -> List[Dict]:
        """
        åŠ è½½æ•°æ®é›†å¹¶è§£ææ–‡ä»¶åä¸­çš„åæ ‡ä¿¡æ¯
        
        æ–‡ä»¶åæ ¼å¼: Pic0001_Bgx112Bgy97_Sdx32Sdy98_cb7cbd17.png
        """
        image_files = []
        
        # å®šä¹‰æ–‡ä»¶åè§£æçš„æ­£åˆ™è¡¨è¾¾å¼
        pattern = r"Pic(\d+)_Bgx(\d+)Bgy(\d+)_Sdx(\d+)Sdy(\d+)_(\w+)\.png"
        
        for file_path in sorted(self.data_dir.glob("*.png")):
            match = re.match(pattern, file_path.name)
            if match:
                pic_id, bg_x, bg_y, sd_x, sd_y, hash_val = match.groups()
                
                image_files.append({
                    'path': file_path,
                    'name': file_path.name,
                    'pic_id': int(pic_id),
                    'gap_x': int(bg_x),
                    'gap_y': int(bg_y),
                    'slider_x': int(sd_x),
                    'slider_y': int(sd_y),
                    'hash': hash_val,
                    'sliding_distance': int(bg_x) - int(sd_x)  # çœŸå®æ»‘åŠ¨è·ç¦»
                })
            else:
                print(f"âš ï¸ Cannot parse filename: {file_path.name}")
        
        return image_files
    
    def _find_models(self) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹æ–‡ä»¶"""
        model_files = []
        
        # æŸ¥æ‰¾æ‰€æœ‰.pthæ–‡ä»¶
        for pth_file in sorted(self.checkpoint_dir.glob("*.pth")):
            # è·³è¿‡ä¼˜åŒ–å™¨çŠ¶æ€æ–‡ä»¶
            if "optimizer" not in pth_file.name.lower():
                model_files.append(pth_file)
        
        return model_files
    
    def evaluate_model(self, model_path: Path, threshold_px: int = 5) -> Dict:
        """
        è¯„ä¼°å•ä¸ªæ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            threshold_px: æ­£ç¡®åˆ¤å®šçš„åƒç´ é˜ˆå€¼
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"\nğŸ“Š Evaluating model: {model_path.name}")
        
        # åŠ è½½æ¨¡å‹
        try:
            predictor = CaptchaPredictor(
                model_path=str(model_path),
                device=self.device
            )
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            return None
        
        # è¯„ä¼°æŒ‡æ ‡
        results = {
            'model_name': model_path.name,
            'total_images': len(self.image_files),
            'predictions': [],
            'errors': [],
            'gap_errors': [],
            'slider_errors': [],
            'distance_errors': [],
            'processing_times': []
        }
        
        # é€å›¾è¯„ä¼°
        for img_info in tqdm(self.image_files, desc="Evaluation Progress"):
            try:
                # é¢„æµ‹
                pred = predictor.predict(str(img_info['path']))
                
                if pred['success']:
                    # è®¡ç®—è¯¯å·®
                    gap_error = np.sqrt(
                        (pred['gap_x'] - img_info['gap_x'])**2 + 
                        (pred['gap_y'] - img_info['gap_y'])**2
                    )
                    slider_error = np.sqrt(
                        (pred['slider_x'] - img_info['slider_x'])**2 + 
                        (pred['slider_y'] - img_info['slider_y'])**2
                    )
                    distance_error = abs(pred['sliding_distance'] - img_info['sliding_distance'])
                    
                    # è®°å½•ç»“æœ
                    results['predictions'].append({
                        'image': img_info['name'],
                        'gap_error': gap_error,
                        'slider_error': slider_error,
                        'distance_error': distance_error,
                        'gap_correct': gap_error <= threshold_px,
                        'slider_correct': slider_error <= threshold_px,
                        'distance_correct': distance_error <= threshold_px,
                        'confidence': pred['confidence'],
                        'processing_time': pred['processing_time_ms']
                    })
                    
                    results['gap_errors'].append(gap_error)
                    results['slider_errors'].append(slider_error)
                    results['distance_errors'].append(distance_error)
                    results['processing_times'].append(pred['processing_time_ms'])
                else:
                    results['errors'].append({
                        'image': img_info['name'],
                        'error': pred.get('error', 'Unknown error')
                    })
            
            except Exception as e:
                results['errors'].append({
                    'image': img_info['name'],
                    'error': str(e)
                })
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        if results['gap_errors']:
            results['metrics'] = self._calculate_metrics(results, threshold_px)
        else:
            results['metrics'] = {'error': 'æ‰€æœ‰é¢„æµ‹å¤±è´¥'}
        
        return results
    
    def _calculate_metrics(self, results: Dict, threshold_px: int) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        gap_errors = np.array(results['gap_errors'])
        slider_errors = np.array(results['slider_errors'])
        distance_errors = np.array(results['distance_errors'])
        
        metrics = {
            # å¹³å‡è¯¯å·®
            'gap_mae': np.mean(gap_errors),
            'slider_mae': np.mean(slider_errors),
            'distance_mae': np.mean(distance_errors),
            
            # ä¸­ä½æ•°è¯¯å·®
            'gap_median': np.median(gap_errors),
            'slider_median': np.median(slider_errors),
            'distance_median': np.median(distance_errors),
            
            # æœ€å¤§è¯¯å·®
            'gap_max': np.max(gap_errors),
            'slider_max': np.max(slider_errors),
            'distance_max': np.max(distance_errors),
            
            # æ­£ç¡®ç‡ï¼ˆé˜ˆå€¼å†…ï¼‰
            'gap_accuracy': np.mean(gap_errors <= threshold_px) * 100,
            'slider_accuracy': np.mean(slider_errors <= threshold_px) * 100,
            'distance_accuracy': np.mean(distance_errors <= threshold_px) * 100,
            
            # ä¸åŒé˜ˆå€¼çš„æ­£ç¡®ç‡
            'hit_le_2px': np.mean(distance_errors <= 2) * 100,
            'hit_le_3px': np.mean(distance_errors <= 3) * 100,
            'hit_le_5px': np.mean(distance_errors <= 5) * 100,
            'hit_le_10px': np.mean(distance_errors <= 10) * 100,
            
            # å¤„ç†æ—¶é—´
            'avg_time_ms': np.mean(results['processing_times']),
            'median_time_ms': np.median(results['processing_times']),
            
            # å¤±è´¥ç‡
            'failure_rate': len(results['errors']) / results['total_images'] * 100
        }
        
        return metrics
    
    def evaluate_all_models(self, threshold_px: int = 5) -> pd.DataFrame:
        """
        è¯„ä¼°æ‰€æœ‰æ¨¡å‹å¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        
        Args:
            threshold_px: æ­£ç¡®åˆ¤å®šçš„åƒç´ é˜ˆå€¼
        
        Returns:
            åŒ…å«æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœçš„DataFrame
        """
        all_results = []
        
        for model_path in self.model_files:
            result = self.evaluate_model(model_path, threshold_px)
            if result and 'metrics' in result:
                # æå–epochç¼–å·
                epoch_match = re.search(r'epoch_(\d+)', model_path.name)
                epoch = int(epoch_match.group(1)) if epoch_match else -1
                
                # æ„å»ºç»“æœè¡Œ
                row = {
                    'model': model_path.name,
                    'epoch': epoch,
                    **result['metrics']
                }
                all_results.append(row)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(all_results)
        
        # æŒ‰epochæ’åº
        if 'epoch' in df.columns:
            df = df.sort_values('epoch')
        
        return df
    
    def generate_report(self, df: pd.DataFrame, output_dir: str = "outputs/real_captchas_report_1.1.0"):
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            df: è¯„ä¼°ç»“æœDataFrame
            output_dir: è¾“å‡ºç›®å½•
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ä¿å­˜CSVæŠ¥å‘Š
        csv_path = output_dir / f"model_evaluation_{timestamp}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8')
        print(f"âœ… CSV report saved: {csv_path}")
        
        # 2. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        self._generate_plots(df, output_dir, timestamp)
        
        # 3. ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        self._generate_text_report(df, output_dir, timestamp)
    
    def _generate_plots(self, df: pd.DataFrame, output_dir: Path, timestamp: str):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        plt.style.use('seaborn-v0_8-darkgrid')
        
        # åˆ›å»ºå›¾è¡¨ï¼šæ­£ç¡®ç‡éšepochå˜åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # å›¾1ï¼š5pxæ­£ç¡®ç‡
        if 'epoch' in df.columns and df['epoch'].min() >= 0:
            ax = axes[0, 0]
            ax.plot(df['epoch'], df['hit_le_5px'], marker='o', linewidth=2, markersize=8)
            ax.set_xlabel('Epoch', fontsize=12)
            ax.set_ylabel('5px Accuracy (%)', fontsize=12)
            ax.set_title('5px Accuracy over Training Epochs', fontsize=14, fontweight='bold')
            ax.grid(True, alpha=0.3)
            
            # æ ‡è®°æœ€ä½³ç‚¹
            best_idx = df['hit_le_5px'].idxmax()
            best_epoch = df.loc[best_idx, 'epoch']
            best_acc = df.loc[best_idx, 'hit_le_5px']
            ax.scatter(best_epoch, best_acc, color='red', s=200, zorder=5)
            ax.annotate(f'Best: {best_acc:.1f}%\nEpoch {best_epoch}',
                       xy=(best_epoch, best_acc),
                       xytext=(10, 10), textcoords='offset points',
                       bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),
                       arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))
        
        # å›¾2ï¼šä¸åŒé˜ˆå€¼çš„æ­£ç¡®ç‡å¯¹æ¯”
        ax = axes[0, 1]
        thresholds = ['hit_le_2px', 'hit_le_3px', 'hit_le_5px', 'hit_le_10px']
        threshold_labels = ['â‰¤2px', 'â‰¤3px', 'â‰¤5px', 'â‰¤10px']
        
        if all(col in df.columns for col in thresholds):
            # é€‰æ‹©æœ€ä½³æ¨¡å‹
            best_model_idx = df['hit_le_5px'].idxmax()
            best_model = df.loc[best_model_idx]
            
            values = [best_model[th] for th in thresholds]
            bars = ax.bar(threshold_labels, values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
            ax.set_ylabel('Accuracy (%)', fontsize=12)
            ax.set_title(f'Best Model Accuracy at Different Thresholds\n({best_model["model"]})', fontsize=14, fontweight='bold')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, val in zip(bars, values):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                       f'{val:.1f}%', ha='center', fontsize=10)
        
        # å›¾3ï¼šå¹³å‡è¯¯å·®å¯¹æ¯”
        ax = axes[1, 0]
        if 'distance_mae' in df.columns:
            ax.plot(df.index, df['distance_mae'], marker='s', linewidth=2, markersize=8, label='Distance MAE')
            ax.plot(df.index, df['gap_mae'], marker='^', linewidth=2, markersize=8, label='Gap MAE')
            ax.plot(df.index, df['slider_mae'], marker='v', linewidth=2, markersize=8, label='Slider MAE')
            ax.set_xlabel('Model Index', fontsize=12)
            ax.set_ylabel('MAE (pixels)', fontsize=12)
            ax.set_title('Mean Absolute Error Comparison', fontsize=14, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        # å›¾4ï¼šå¤„ç†æ—¶é—´
        ax = axes[1, 1]
        if 'avg_time_ms' in df.columns:
            ax.bar(range(len(df)), df['avg_time_ms'], color='#95A5A6')
            ax.set_xlabel('Model Index', fontsize=12)
            ax.set_ylabel('Avg Time (ms)', fontsize=12)
            ax.set_title('Average Processing Time', fontsize=14, fontweight='bold')
            ax.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plot_path = output_dir / f"model_evaluation_plots_{timestamp}.png"
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ… Visualization report saved: {plot_path}")
    
    def _generate_text_report(self, df: pd.DataFrame, output_dir: Path, timestamp: str):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("çœŸå®éªŒè¯ç æ•°æ®é›†æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"æ•°æ®é›†è·¯å¾„: {self.data_dir}")
        report_lines.append(f"æ¨¡å‹ç›®å½•: {self.checkpoint_dir}")
        report_lines.append(f"è¯„ä¼°å›¾ç‰‡æ•°: {len(self.image_files)}")
        report_lines.append("")
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        if 'hit_le_5px' in df.columns:
            best_idx = df['hit_le_5px'].idxmax()
            best_model = df.loc[best_idx]
            
            report_lines.append("ğŸ† æœ€ä½³æ¨¡å‹ï¼ˆ5pxæ­£ç¡®ç‡ï¼‰")
            report_lines.append("-" * 40)
            report_lines.append(f"æ¨¡å‹: {best_model['model']}")
            report_lines.append(f"5pxæ­£ç¡®ç‡: {best_model['hit_le_5px']:.2f}%")
            report_lines.append(f"æ»‘åŠ¨è·ç¦»MAE: {best_model['distance_mae']:.2f} px")
            report_lines.append(f"å¹³å‡å¤„ç†æ—¶é—´: {best_model['avg_time_ms']:.2f} ms")
            report_lines.append("")
        
        # æ‰€æœ‰æ¨¡å‹çš„è¯¦ç»†ç»“æœ
        report_lines.append("ğŸ“Š æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœ")
        report_lines.append("-" * 80)
        
        # æ„å»ºè¡¨æ ¼
        columns_to_show = ['model', 'hit_le_5px', 'distance_mae', 'gap_mae', 'slider_mae', 'avg_time_ms']
        available_columns = [col for col in columns_to_show if col in df.columns]
        
        # æ ¼å¼åŒ–DataFrameä¸ºå­—ç¬¦ä¸²
        df_display = df[available_columns].copy()
        
        # æ ¼å¼åŒ–æ•°å€¼åˆ—
        for col in df_display.columns:
            if col != 'model':
                df_display[col] = df_display[col].apply(lambda x: f"{x:.2f}" if pd.notna(x) else "N/A")
        
        report_lines.append(df_display.to_string(index=False))
        report_lines.append("")
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        txt_path = output_dir / f"model_evaluation_report_{timestamp}.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"âœ… Text report saved: {txt_path}")
        
        # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
        print("\n" + "\n".join(report_lines))


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è¯„ä¼°æ¨¡å‹åœ¨çœŸå®éªŒè¯ç æ•°æ®é›†ä¸Šçš„è¡¨ç°')
    parser.add_argument('--data-dir', type=str, 
                       default='data/real_captchas/annotated',
                       help='çœŸå®éªŒè¯ç æ•°æ®ç›®å½•')
    parser.add_argument('--checkpoint-dir', type=str,
                       default='src/checkpoints/1.1.0',
                       help='æ¨¡å‹checkpointç›®å½•')
    parser.add_argument('--threshold', type=int, default=5,
                       help='æ­£ç¡®åˆ¤å®šçš„åƒç´ é˜ˆå€¼')
    parser.add_argument('--device', type=str, default='auto',
                       choices=['auto', 'cuda', 'cpu'],
                       help='è¿è¡Œè®¾å¤‡')
    parser.add_argument('--output-dir', type=str, default='outputs/real_captchas_report_1.1.0',
                       help='æŠ¥å‘Šè¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RealCaptchaEvaluator(
        data_dir=args.data_dir,
        checkpoint_dir=args.checkpoint_dir,
        device=args.device
    )
    
    # è¯„ä¼°æ‰€æœ‰æ¨¡å‹
    print("\nğŸš€ Starting evaluation of all models...")
    results_df = evaluator.evaluate_all_models(threshold_px=args.threshold)
    
    # ç”ŸæˆæŠ¥å‘Š
    if not results_df.empty:
        print("\nğŸ“ Generating evaluation report...")
        evaluator.generate_report(results_df, args.output_dir)
        print("\nâœ… Evaluation completed!")
    else:
        print("\nâŒ No models were successfully evaluated")


if __name__ == "__main__":
    main()