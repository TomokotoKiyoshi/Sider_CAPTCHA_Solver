# -*- coding: utf-8 -*-
"""
æµå¼æ•°æ®é›†ç”Ÿæˆå™¨
ä½¿ç”¨å¯å¤ç”¨æ‰¹ç¼“å†²åŒºï¼Œé¿å…å†…å­˜ç´¯ç§¯
"""
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Any
from tqdm import tqdm
from datetime import datetime
from multiprocessing import Pool
import warnings
import gc

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')

# å…¨å±€é¢„å¤„ç†å™¨å®ä¾‹ï¼ˆæ¯ä¸ªè¿›ç¨‹ä¸€ä¸ªï¼‰
_global_preprocessor = None

def init_worker(config: Dict):
    """
    åˆå§‹åŒ–å·¥ä½œè¿›ç¨‹ï¼Œåˆ›å»ºé¢„å¤„ç†å™¨å®ä¾‹
    æ¯ä¸ªè¿›ç¨‹åªåˆ›å»ºä¸€æ¬¡ï¼Œé¿å…é‡å¤åˆå§‹åŒ–å¼€é”€
    """
    global _global_preprocessor
    from .preprocessor import TrainingPreprocessor
    _global_preprocessor = TrainingPreprocessor(config)

def process_single_sample_optimized(label: Dict) -> Optional[Dict[str, Any]]:
    """
    å¤„ç†å•ä¸ªæ ·æœ¬çš„ä¼˜åŒ–å‡½æ•°
    ä½¿ç”¨å…¨å±€é¢„å¤„ç†å™¨ï¼Œé¿å…é‡å¤åˆ›å»º
    """
    try:
        # ä½¿ç”¨å…¨å±€é¢„å¤„ç†å™¨
        global _global_preprocessor
        if _global_preprocessor is None:
            return None
            
        # æ„å»ºå›¾åƒè·¯å¾„ï¼ˆä½¿ç”¨labelä¸­çš„data_rootï¼‰
        data_root = Path(label['_data_root'])
        image_path = data_root / label['paths']['composite']
        
        if not image_path.exists():
            return None
        
        # æå–åæ ‡ä¿¡æ¯
        gap_center = tuple(label['labels']['bg_gap_center'])
        slider_center = tuple(label['labels']['comp_piece_center'])
        gap_angle = label['labels']['gap_pose'].get('delta_theta_deg', 0.0)
        
        # å¤„ç†æ··æ·†ç¼ºå£
        fake_gaps = []
        if 'fake_gaps' in label['labels']:
            for fake_gap in label['labels']['fake_gaps']:
                fake_gaps.append(tuple(fake_gap['center']))
        
        # ä½¿ç”¨é¢„å¤„ç†å™¨å¤„ç†å›¾åƒ
        result = _global_preprocessor.preprocess(
            str(image_path),
            gap_center=gap_center,
            slider_center=slider_center,
            confusing_gaps=fake_gaps if fake_gaps else None,
            gap_angle=gap_angle
        )
        
        # åªè¿”å›å¿…è¦çš„æ•°æ®ï¼Œå‡å°‘åºåˆ—åŒ–å¼€é”€
        # æ³¨æ„ï¼šresultä¸­çš„æ•°æ®å·²ç»æ˜¯NumPyæ•°ç»„ï¼Œä¸éœ€è¦å†è°ƒç”¨.numpy()
        return {
            'sample_id': label.get('sample_id', 'unknown'),
            'input': result['input'],  # å·²ç»æ˜¯numpyæ•°ç»„
            'heatmaps': result['heatmaps'],  # å·²ç»æ˜¯numpyæ•°ç»„
            'offsets': result['offsets'],  # å·²ç»æ˜¯numpyæ•°ç»„
            'weight_mask': result['weight_mask'],  # å·²ç»æ˜¯numpyæ•°ç»„
            'transform_params': result['transform_params'],
            'gap_grid': result['gap_grid'],
            'gap_offset': result['gap_offset'],
            'slider_grid': result['slider_grid'],
            'slider_offset': result['slider_offset'],
            'confusing_gaps': result.get('confusing_gaps', []),
            'gap_angle': result.get('gap_angle', 0.0)
        }
    except Exception as e:
        print(f"Failed to process sample {label.get('sample_id', 'unknown')}: {e}")
        return None


class StreamingDatasetGenerator:
    """
    æµå¼æ•°æ®é›†ç”Ÿæˆå™¨
    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. ä½¿ç”¨å¯å¤ç”¨çš„æ‰¹ç¼“å†²åŒºï¼Œé¿å…å†…å­˜ç´¯ç§¯
    2. æ‰¹æ»¡å³å†™ç›˜ï¼Œæ²¡æœ‰ä¸­é—´åˆ—è¡¨
    3. åˆ†æ–‡ä»¶ä¿å­˜é¿å…ä¸´æ—¶zipç¼“å†²
    4. å†…å­˜ä½¿ç”¨ç¨³å®šä¸”å¯é¢„æµ‹
    """
    
    def __init__(self,
                 data_root: str,
                 output_dir: str,
                 config_path: Optional[str] = None,
                 batch_size: int = None,
                 num_workers: Optional[int] = None):
        """
        åˆå§‹åŒ–æµå¼æ•°æ®é›†ç”Ÿæˆå™¨
        """
        from .config_loader import load_config
        from multiprocessing import cpu_count
        
        self.data_root = Path(data_root)
        self.output_dir = Path(output_dir)
        
        # åŠ è½½é…ç½®
        self.config = load_config(config_path)
        preprocessing_config = self.config['preprocessing']
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–batch_size
        if batch_size is not None:
            self.batch_size = batch_size
        elif 'dataset' in self.config and 'batch_size' in self.config['dataset']:
            self.batch_size = self.config['dataset']['batch_size']
        else:
            self.batch_size = 64
        
        # è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°
        if num_workers is not None:
            self.num_workers = num_workers
        elif 'dataset' in self.config and 'num_workers' in self.config['dataset']:
            self.num_workers = self.config['dataset']['num_workers']
            print(f"Using num_workers from config: {self.num_workers}")
        else:
            self.num_workers = max(1, cpu_count() - 1)
            print(f"Using default num_workers: {self.num_workers} (CPU cores - 1)")
        
        # ä»é…ç½®è¯»å–è¾“å‡ºç»“æ„
        self.output_structure = self.config.get('output_structure', {
            'image_subdir': 'images',
            'label_subdir': 'labels'
        })
        self.file_naming = self.config.get('file_naming', {
            'image_pattern': '{split}_{batch_id:04d}.npy',
            'heatmap_pattern': '{split}_{batch_id:04d}_heatmaps.npy',
            'offset_pattern': '{split}_{batch_id:04d}_offsets.npy',
            'weight_pattern': '{split}_{batch_id:04d}_weights.npy',
            'meta_pattern': '{split}_{batch_id:04d}_meta.json',
            'index_pattern': '{split}_index.json'
        })
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„splitç›®å½•ç»“æ„ï¼‰
        self.output_dir.mkdir(parents=True, exist_ok=True)
        splits_config = self.output_structure.get('splits', {
            'train': 'train',
            'val': 'val', 
            'test': 'test'
        })
        
        for split_key, split_dir in splits_config.items():
            (self.output_dir / self.output_structure['image_subdir'] / split_dir).mkdir(parents=True, exist_ok=True)
            (self.output_dir / self.output_structure['label_subdir'] / split_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºsplit_infoç›®å½•ï¼ˆå¦‚æœé…ç½®ä¸­æœ‰ï¼‰
        if 'split_info_subdir' in self.output_structure:
            (self.output_dir / self.output_structure['split_info_subdir']).mkdir(parents=True, exist_ok=True)
        
        # è®°å½•é…ç½®ä¿¡æ¯
        self.target_size = preprocessing_config['letterbox']['target_size']
        self.downsample = preprocessing_config['coordinate']['downsample']
        
        # è®¡ç®—è¾“å‡ºç»´åº¦
        self.input_shape = (4, self.target_size[1], self.target_size[0])
        self.grid_size = (
            self.target_size[1] // self.downsample,
            self.target_size[0] // self.downsample
        )
        
        # åˆå§‹åŒ–å¯å¤ç”¨çš„æ‰¹ç¼“å†²åŒº
        self._init_batch_buffers()
        
        print(f"Streaming dataset generator initialized")
        print(f"  Data root: {self.data_root}")
        print(f"  Output dir: {self.output_dir}")
        print(f"  Batch size: {self.batch_size}")
        print(f"  Worker processes: {self.num_workers}")
        print(f"  Target size: {self.target_size}")
        print(f"  Grid size: {self.grid_size}")
        print(f"  âœ… Using streaming write with reusable buffers")
    
    def _init_batch_buffers(self):
        """åˆå§‹åŒ–å¯å¤ç”¨çš„æ‰¹ç¼“å†²åŒº"""
        # é¢„åˆ†é…æ‰¹ç¼“å†²åŒºï¼Œé¿å…é‡å¤åˆ†é…
        self._buf_images = np.empty((self.batch_size, *self.input_shape), dtype=np.float32)
        self._buf_heatmaps = np.empty((self.batch_size, 2, *self.grid_size), dtype=np.float32)
        self._buf_offsets = np.empty((self.batch_size, 4, *self.grid_size), dtype=np.float32)
        self._buf_weight_masks = np.empty((self.batch_size, *self.grid_size), dtype=np.float32)
        
        # å…ƒæ•°æ®ç¼“å†²ï¼ˆè¿™äº›æ¯”è¾ƒå°ï¼Œå¯ä»¥ç”¨åˆ—è¡¨ï¼‰
        self._buf_metadata = {
            'sample_ids': [],
            'transform_params': [],
            'grid_coords': [],
            'offsets_meta': [],
            'confusing_gaps': [],
            'gap_angles': []
        }
        
        # å†™å…¥æŒ‡é’ˆ
        self._buf_ptr = 0
        self._batch_counter = 0
    
    def _reset_metadata_buffer(self):
        """é‡ç½®å…ƒæ•°æ®ç¼“å†²"""
        self._buf_metadata = {
            'sample_ids': [],
            'transform_params': [],
            'grid_coords': [],
            'offsets_meta': [],
            'confusing_gaps': [],
            'gap_angles': []
        }
    
    def _write_sample_to_buffer(self, sample: Dict[str, Any]) -> bool:
        """
        å°†æ ·æœ¬å†™å…¥ç¼“å†²åŒº
        
        Returns:
            True if buffer is full and needs to be flushed
        """
        idx = self._buf_ptr
        
        # ç›´æ¥å†™å…¥é¢„åˆ†é…çš„æ•°ç»„ç¼“å†²åŒº
        self._buf_images[idx] = sample['input']
        self._buf_heatmaps[idx] = sample['heatmaps']
        self._buf_offsets[idx] = sample['offsets']
        self._buf_weight_masks[idx] = sample['weight_mask']
        
        # å…ƒæ•°æ®æ·»åŠ åˆ°åˆ—è¡¨
        self._buf_metadata['sample_ids'].append(sample['sample_id'])
        self._buf_metadata['transform_params'].append(sample['transform_params'])
        self._buf_metadata['grid_coords'].append({
            'gap': sample['gap_grid'],
            'slider': sample['slider_grid']
        })
        self._buf_metadata['offsets_meta'].append({
            'gap': sample['gap_offset'],
            'slider': sample['slider_offset']
        })
        self._buf_metadata['confusing_gaps'].append(sample.get('confusing_gaps', []))
        self._buf_metadata['gap_angles'].append(sample.get('gap_angle', 0.0))
        
        # ç§»åŠ¨æŒ‡é’ˆ
        self._buf_ptr += 1
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å†²åŒº
        return self._buf_ptr >= self.batch_size
    
    def _flush_buffer_to_disk(self, split: str):
        """å°†å½“å‰ç¼“å†²åŒºå†™å…¥ç£ç›˜"""
        if self._buf_ptr == 0:
            return  # ç©ºç¼“å†²åŒºï¼Œæ— éœ€å†™å…¥
        
        batch_size = self._buf_ptr
        batch_id = self._batch_counter
        
        # ä½¿ç”¨é…ç½®çš„æ–‡ä»¶å‘½åæ¨¡å¼å‡†å¤‡æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„splitç›®å½•ï¼‰
        splits_config = self.output_structure.get('splits', {
            'train': 'train',
            'val': 'val',
            'test': 'test'
        })
        split_dir = splits_config.get(split, split)  # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨åŸå§‹splitåç§°
        
        image_path = self.output_dir / self.output_structure['image_subdir'] / split_dir / self.file_naming['image_pattern'].format(split=split, batch_id=batch_id)
        heatmap_path = self.output_dir / self.output_structure['label_subdir'] / split_dir / self.file_naming['heatmap_pattern'].format(split=split, batch_id=batch_id)
        offset_path = self.output_dir / self.output_structure['label_subdir'] / split_dir / self.file_naming['offset_pattern'].format(split=split, batch_id=batch_id)
        weight_path = self.output_dir / self.output_structure['label_subdir'] / split_dir / self.file_naming['weight_pattern'].format(split=split, batch_id=batch_id)
        meta_path = self.output_dir / self.output_structure['label_subdir'] / split_dir / self.file_naming['meta_pattern'].format(split=split, batch_id=batch_id)
        
        # åˆ†æ–‡ä»¶ä¿å­˜ï¼Œé¿å…np.savezçš„ä¸´æ—¶zipç¼“å†²
        # åªä¿å­˜å®é™…ä½¿ç”¨çš„éƒ¨åˆ†ï¼ˆ0:batch_sizeï¼‰
        np.save(image_path, self._buf_images[:batch_size])
        np.save(heatmap_path, self._buf_heatmaps[:batch_size])
        np.save(offset_path, self._buf_offsets[:batch_size])
        np.save(weight_path, self._buf_weight_masks[:batch_size])
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'batch_id': batch_id,
            'batch_size': batch_size,
            **self._buf_metadata
        }
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"  âœ… Batch {batch_id} saved: {batch_size} samples")
        
        # é‡ç½®ç¼“å†²åŒºæŒ‡é’ˆå’Œå…ƒæ•°æ®
        self._buf_ptr = 0
        self._reset_metadata_buffer()
        self._batch_counter += 1
    
    def load_labels(self, labels_path: str) -> List[Dict]:
        """åŠ è½½æ ‡ç­¾æ–‡ä»¶"""
        with open(labels_path, 'r', encoding='utf-8') as f:
            labels = json.load(f)
        print(f"Loaded {len(labels)} labels")
        
        # åœ¨æ ‡ç­¾ä¸­æ·»åŠ data_rootè·¯å¾„ï¼Œé¿å…åºåˆ—åŒ–æ•´ä¸ªè·¯å¾„å¯¹è±¡
        for label in labels:
            label['_data_root'] = str(self.data_root)
        
        return labels
    
    def generate_dataset(self, labels_path: str, split: str = 'train', max_samples: Optional[int] = None,
                        labels_subset: Optional[List[Dict]] = None):
        """
        ä½¿ç”¨æµå¼å†™å…¥ç”Ÿæˆæ•°æ®é›†
        
        Args:
            labels_path: æ ‡ç­¾æ–‡ä»¶è·¯å¾„
            split: æ•°æ®é›†åˆ’åˆ† (train/val/test)
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            labels_subset: é¢„å…ˆåˆ’åˆ†å¥½çš„æ ‡ç­¾å­é›†ï¼ˆå¦‚æœæä¾›ï¼Œåˆ™å¿½ç•¥labels_pathï¼‰
        """
        print(f"\n{'='*60}")
        print(f"Generating {split} dataset (Streaming Version)")
        print(f"{'='*60}")
        print(f"Using {self.num_workers} parallel processes")
        
        # åŠ è½½æ ‡ç­¾
        if labels_subset is not None:
            labels = labels_subset
            print(f"Using provided subset: {len(labels)} labels")
            # ä¸ºsubsetæ¨¡å¼ä¸‹çš„æ ‡ç­¾ä¹Ÿæ·»åŠ _data_rootå­—æ®µ
            for label in labels:
                if '_data_root' not in label:
                    label['_data_root'] = str(self.data_root)
        else:
            labels = self.load_labels(labels_path)
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ ·æœ¬æ•°ï¼Œæˆªå–æ ‡ç­¾
        if max_samples is not None and max_samples < len(labels):
            labels = labels[:max_samples]
            print(f"Limiting to {max_samples} samples for testing")
        
        # é‡ç½®ç¼“å†²åŒº
        self._buf_ptr = 0
        self._batch_counter = 0
        self._reset_metadata_buffer()
        
        failed_count = 0
        
        # ä»é…ç½®è¯»å–å‚æ•°
        chunk_size_config = self.config.get('dataset', {}).get('chunk_size', 20)
        maxtasksperchild = self.config.get('dataset', {}).get('maxtasksperchild', 10)
        gc_interval = self.config.get('dataset', {}).get('gc_interval', 2)
        
        print(f"Configuration:")
        print(f"  Chunk size: {chunk_size_config}")
        print(f"  Max tasks per child: {maxtasksperchild}")
        print(f"  GC interval: every {gc_interval} batches")
        print(f"  ğŸ’¡ Streaming write enabled - no memory accumulation")
        
        # åˆ›å»ºè¿›ç¨‹æ± 
        with Pool(
            processes=self.num_workers,
            initializer=init_worker,
            initargs=(self.config,),
            maxtasksperchild=maxtasksperchild
        ) as pool:
            
            with tqdm(total=len(labels), desc=f"Processing {split} samples") as pbar:
                # ä½¿ç”¨imap_unorderedæµå¼å¤„ç†
                for processed in pool.imap_unordered(
                    process_single_sample_optimized,
                    labels,
                    chunksize=chunk_size_config
                ):
                    if processed is not None:
                        # ç›´æ¥å†™å…¥ç¼“å†²åŒº
                        if self._write_sample_to_buffer(processed):
                            # ç¼“å†²åŒºæ»¡ï¼Œç«‹å³å†™ç›˜
                            self._flush_buffer_to_disk(split)
                            
                            # å®šæœŸå¼ºåˆ¶åƒåœ¾å›æ”¶
                            if self._batch_counter % gc_interval == 0:
                                gc.collect(2)  # å®Œæ•´åƒåœ¾å›æ”¶
                    else:
                        failed_count += 1
                    
                    pbar.update(1)
        
        # ä¿å­˜å‰©ä½™çš„æ ·æœ¬ï¼ˆæœªæ»¡ä¸€æ‰¹çš„ï¼‰
        if self._buf_ptr > 0:
            self._flush_buffer_to_disk(split)
        
        print(f"\n{'='*60}")
        print(f"Dataset generation completed:")
        print(f"  âœ… Success: {len(labels) - failed_count}")
        print(f"  âŒ Failed: {failed_count}")
        print(f"  ğŸ“¦ Total batches: {self._batch_counter}")
        print(f"{'='*60}")
        
        # ç”Ÿæˆç´¢å¼•æ–‡ä»¶
        if len(labels) - failed_count > 0:
            self._generate_index(split)
    
    def _generate_index(self, split: str):
        """ç”Ÿæˆæ•°æ®é›†ç´¢å¼•æ–‡ä»¶"""
        image_files = sorted((self.output_dir / self.output_structure['image_subdir'] / split).glob(f'{split}_*.npy'))
        
        index = {
            'split': split,
            'num_batches': len(image_files),
            'batch_size': self.batch_size,
            'total_samples': 0,
            'input_shape': self.input_shape,
            'grid_size': self.grid_size,
            'batches': []
        }
        
        for img_file in image_files:
            batch_id = int(img_file.stem.split('_')[-1])
            meta_file = self.output_dir / self.output_structure['label_subdir'] / split / self.file_naming['meta_pattern'].format(split=split, batch_id=batch_id)
            
            with open(meta_file, 'r', encoding='utf-8') as f:
                meta = json.load(f)
            
            batch_info = {
                'batch_id': batch_id,
                'batch_size': meta['batch_size'],
                'image_file': img_file.name,
                'heatmap_file': self.file_naming['heatmap_pattern'].format(split=split, batch_id=batch_id),
                'offset_file': self.file_naming['offset_pattern'].format(split=split, batch_id=batch_id),
                'weight_file': self.file_naming['weight_pattern'].format(split=split, batch_id=batch_id),
                'meta_file': meta_file.name
            }
            index['batches'].append(batch_info)
            index['total_samples'] += meta['batch_size']
        
        # ä¿å­˜ç´¢å¼•ï¼ˆä¿å­˜åˆ°å¯¹åº”çš„splitæ–‡ä»¶å¤¹ï¼‰
        index_path = self.output_dir / self.output_structure['label_subdir'] / split / self.file_naming['index_pattern'].format(split=split)
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index, f, indent=2)
        
        print(f"\nğŸ“‹ Generated index file: {index_path}")
        print(f"  Total batches: {len(image_files)}")
        print(f"  Total samples: {index['total_samples']}")