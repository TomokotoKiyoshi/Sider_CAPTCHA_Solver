"""é«˜å±‚é¢„æµ‹å™¨APIï¼Œå°è£…åº•å±‚æ¨¡å‹è°ƒç”¨"""

import torch
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, Optional, Union, List
from tqdm import tqdm
import time

# å¯¼å…¥srcä¸­çš„æ¨¡å‹ä»£ç 
import sys
package_dir = Path(__file__).parent.parent
src_path = package_dir / 'src'
if src_path.exists() and str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from src.models.lite_hrnet_18_fpn import create_lite_hrnet_18_fpn
from src.preprocessing.preprocessor import LetterboxTransform, CoordinateTransform
from src.config.config_loader import ConfigLoader


class CaptchaPredictor:
    """æ»‘å—éªŒè¯ç é¢„æµ‹å™¨
    
    æä¾›é«˜å±‚APIç”¨äºæ»‘å—éªŒè¯ç è¯†åˆ«ï¼Œè‡ªåŠ¨å¤„ç†æ¨¡å‹åŠ è½½ã€å›¾åƒé¢„å¤„ç†å’Œç»“æœåå¤„ç†ã€‚
    """
    
    def __init__(self, 
                 model_path: str = 'best',
                 device: str = 'auto',
                 hm_threshold: float = 0.1):
        """åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„æˆ–é¢„è®¾åç§°('best'/'v1.1.0')
            device: è¿è¡Œè®¾å¤‡ ('auto'/'cuda'/'cpu')
            hm_threshold: çƒ­åŠ›å›¾é˜ˆå€¼ï¼Œç”¨äºç­›é€‰æœ‰æ•ˆæ£€æµ‹
        """
        self.device = self._setup_device(device)
        self.threshold = hm_threshold
        
        # åŠ è½½é…ç½®
        self.config_loader = ConfigLoader()
        self.model_config = self.config_loader.get_config('model_config')
        
        # åˆå§‹åŒ–é¢„å¤„ç†å™¨ - ä½¿ç”¨æ­£ç¡®çš„ 512x256 å°ºå¯¸
        self.letterbox = LetterboxTransform(
            target_size=(512, 256),  # å®½xé«˜
            fill_value=255  # ç™½è‰²å¡«å……
        )
        self.coord_transform = CoordinateTransform(downsample=4)
        
        # åŠ è½½æ¨¡å‹
        self.model = create_lite_hrnet_18_fpn()
        
        # åŠ è½½æƒé‡
        weights_path = self._get_model_path(model_path)
        self._load_weights(weights_path)
        
        # å°†æ¨¡å‹ç§»åˆ°æŒ‡å®šè®¾å¤‡
        self.model.to(self.device)
        self.model.eval()
        
        # éªŒè¯è®¾å¤‡å¹¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if str(self.device) == 'cuda':
            # ç¡®è®¤æ¨¡å‹åœ¨GPUä¸Š
            if next(self.model.parameters()).is_cuda:
                print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ | è®¾å¤‡: GPU ({torch.cuda.get_device_name(0)})")
                # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨
                allocated = torch.cuda.memory_allocated() / 1024**2
                print(f"   GPUå†…å­˜ä½¿ç”¨: {allocated:.1f} MB")
            else:
                print(f"âš ï¸ æ¨¡å‹åŠ è½½æˆåŠŸä½†æœªåœ¨GPUä¸Š | è®¾å¤‡: {self.device}")
        else:
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ | è®¾å¤‡: CPU")
    
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è¿è¡Œè®¾å¤‡"""
        if device == 'auto':
            return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        return torch.device(device)
    
    def _get_model_path(self, model_path: str) -> Path:
        """è·å–æ¨¡å‹è·¯å¾„ï¼Œæ”¯æŒæœ¬åœ°è·¯å¾„å’Œé¢„è®¾æ¨¡å‹å
        
        ä¼˜å…ˆä½¿ç”¨ best_model_weights.pthï¼ˆæ›´å°ï¼Œæ¨ç†ä¸“ç”¨ï¼‰
        å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨ best_model.pthï¼ˆå®Œæ•´æ£€æŸ¥ç‚¹ï¼‰
        """
        # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ä¸”å­˜åœ¨
        if Path(model_path).exists():
            return Path(model_path)
        
        # å°è¯•åœ¨src/checkpointsä¸­æŸ¥æ‰¾
        checkpoints_dir = Path(__file__).parent.parent / 'src' / 'checkpoints'
        if checkpoints_dir.exists():
            # æŸ¥æ‰¾bestæ¨¡å‹ - é»˜è®¤ä½¿ç”¨1.1.0ç‰ˆæœ¬
            if model_path == 'best':
                # ä¼˜å…ˆæŸ¥æ‰¾1.1.0/best_model_weights.pthï¼ˆæ¨ç†ä¸“ç”¨ï¼Œæ–‡ä»¶æ›´å°ï¼‰
                weights_path = checkpoints_dir / '1.1.0' / 'best_model_weights.pth'
                if weights_path.exists():
                    print(f"   ä½¿ç”¨æ¨ç†æƒé‡: {weights_path.name} (12.96 MB)")
                    return weights_path
                
                # å…¶æ¬¡æŸ¥æ‰¾1.1.0/best_model.pthï¼ˆå®Œæ•´æ£€æŸ¥ç‚¹ï¼‰
                best_path = checkpoints_dir / '1.1.0' / 'best_model.pth'
                if best_path.exists():
                    print(f"   ä½¿ç”¨å®Œæ•´æ£€æŸ¥ç‚¹: {best_path.name} (50.75 MB)")
                    return best_path
                
                # å¦åˆ™æŸ¥æ‰¾ä»»æ„ç‰ˆæœ¬çš„æƒé‡æ–‡ä»¶
                for checkpoint_path in checkpoints_dir.glob('**/best_model_weights.pth'):
                    print(f"   ä½¿ç”¨æ¨ç†æƒé‡: {checkpoint_path.name}")
                    return checkpoint_path
                for checkpoint_path in checkpoints_dir.glob('**/best_model.pth'):
                    print(f"   ä½¿ç”¨å®Œæ•´æ£€æŸ¥ç‚¹: {checkpoint_path.name}")
                    return checkpoint_path
            
            # æŸ¥æ‰¾æŒ‡å®šç‰ˆæœ¬ (æ”¯æŒ "1.1.0" æˆ– "v1.1.0" æ ¼å¼)
            version = model_path.lstrip('v')
            
            # ä¼˜å…ˆå°è¯•weightsç‰ˆæœ¬
            weights_path = checkpoints_dir / version / "best_model_weights.pth"
            if weights_path.exists():
                print(f"   ä½¿ç”¨æ¨ç†æƒé‡: {weights_path.name}")
                return weights_path
            
            # å¦åˆ™ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬
            full_path = checkpoints_dir / version / "best_model.pth"
            if full_path.exists():
                print(f"   ä½¿ç”¨å®Œæ•´æ£€æŸ¥ç‚¹: {full_path.name}")
                return full_path
        
        raise ValueError(f"æ‰¾ä¸åˆ°æ¨¡å‹: {model_path}")
    
    def _load_weights(self, weights_path: Path):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        # ç›´æ¥åŠ è½½åˆ°ç›®æ ‡è®¾å¤‡ä»¥æé«˜æ•ˆç‡
        # å¦‚æœæ˜¯GPUï¼Œç›´æ¥åŠ è½½åˆ°GPUï¼›å¦åˆ™åŠ è½½åˆ°CPU
        if str(self.device) == 'cuda' and torch.cuda.is_available():
            checkpoint = torch.load(weights_path, map_location=self.device)
        else:
            checkpoint = torch.load(weights_path, map_location='cpu')
        
        # å¤„ç†ä¸åŒæ ¼å¼çš„checkpoint
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        self.model.load_state_dict(state_dict)
    
    def predict(self, image_path: Union[str, Path, np.ndarray]) -> Dict:
        """é¢„æµ‹å•å¼ å›¾ç‰‡
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„æˆ–numpyæ•°ç»„
        
        Returns:
            Dict: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸:
                - success: æ˜¯å¦æˆåŠŸæ£€æµ‹
                - sliding_distance: æ»‘åŠ¨è·ç¦»(åƒç´ )
                - gap_x, gap_y: ç¼ºå£ä¸­å¿ƒåæ ‡
                - slider_x, slider_y: æ»‘å—ä¸­å¿ƒåæ ‡
                - gap_confidence: ç¼ºå£ç½®ä¿¡åº¦
                - slider_confidence: æ»‘å—ç½®ä¿¡åº¦
                - confidence: ç»¼åˆç½®ä¿¡åº¦
                - processing_time_ms: å¤„ç†æ—¶é—´(æ¯«ç§’)
        """
        start_time = time.time()
        
        try:
            # é¢„å¤„ç†
            image_tensor = self._preprocess(image_path)
            
            # æ¨ç†
            with torch.no_grad():
                outputs = self.model(image_tensor)
                predictions = self.model.decode_predictions(outputs, input_images=image_tensor)
            
            # åå¤„ç†
            result = self._format_result(predictions)
            result['processing_time_ms'] = (time.time() - start_time) * 1000
            result['success'] = True
            
            return result
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_time_ms': (time.time() - start_time) * 1000
            }
    
    def predict_batch(self, image_paths: List[Union[str, Path]], batch_size: int = 32) -> List[Dict]:
        """æ‰¹é‡é¢„æµ‹å¤šå¼ å›¾ç‰‡ - æ”¯æŒçœŸæ­£çš„æ‰¹é‡GPUæ¨ç†
        
        Args:
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼‰
        
        Returns:
            List[Dict]: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        num_images = len(image_paths)
        
        # åˆ†æ‰¹å¤„ç†
        for batch_start in tqdm(range(0, num_images, batch_size), desc="æ‰¹é‡æ¨ç†"):
            batch_end = min(batch_start + batch_size, num_images)
            batch_paths = image_paths[batch_start:batch_end]
            
            # æ‰¹é‡é¢„å¤„ç†
            batch_tensors = []
            batch_params = []
            batch_original_sizes = []
            valid_indices = []
            
            for idx, image_path in enumerate(batch_paths):
                try:
                    # é¢„å¤„ç†å•å¼ å›¾ç‰‡
                    image_tensor, transform_params, original_size = self._preprocess_with_params(image_path)
                    batch_tensors.append(image_tensor)
                    batch_params.append(transform_params)
                    batch_original_sizes.append(original_size)
                    valid_indices.append(idx)
                except Exception as e:
                    # è®°å½•å¤±è´¥çš„å›¾ç‰‡
                    results.append({
                        'success': False,
                        'error': str(e),
                        'image_path': str(image_path)
                    })
            
            if batch_tensors:
                # åˆå¹¶æˆæ‰¹æ¬¡tensor
                batch_tensor = torch.cat(batch_tensors, dim=0)  # [B, 2, 256, 512]
                
                # æ‰¹é‡æ¨ç†
                with torch.no_grad():
                    outputs = self.model(batch_tensor)
                    predictions = self.model.decode_predictions(outputs, input_images=batch_tensor)
                
                # æ‰¹é‡åå¤„ç†
                for i, idx in enumerate(valid_indices):
                    try:
                        result = self._format_batch_result(
                            predictions, i, batch_params[i], batch_original_sizes[i]
                        )
                        result['success'] = True
                        result['image_path'] = str(batch_paths[idx])
                        results.append(result)
                    except Exception as e:
                        results.append({
                            'success': False,
                            'error': str(e),
                            'image_path': str(batch_paths[idx])
                        })
        
        return results
    
    def predict_batch_folder(self, folder_path: Union[str, Path], 
                            batch_size: int = 32,
                            extensions: List[str] = ['.png', '.jpg', '.jpeg']) -> List[Dict]:
        """æ‰¹é‡é¢„æµ‹æ•´ä¸ªæ–‡ä»¶å¤¹çš„å›¾ç‰‡
        
        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            batch_size: æ‰¹å¤„ç†å¤§å°
            extensions: æ”¯æŒçš„å›¾ç‰‡æ‰©å±•å
        
        Returns:
            List[Dict]: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        folder_path = Path(folder_path)
        
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_paths = []
        for ext in extensions:
            image_paths.extend(folder_path.glob(f'*{ext}'))
            image_paths.extend(folder_path.glob(f'*{ext.upper()}'))
        
        image_paths = sorted(list(set(image_paths)))  # å»é‡å¹¶æ’åº
        
        if not image_paths:
            print(f"âš ï¸ æ–‡ä»¶å¤¹ {folder_path} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡")
            return []
        
        print(f"ğŸ“ æ‰¾åˆ° {len(image_paths)} å¼ å›¾ç‰‡")
        print(f"ğŸš€ ä½¿ç”¨æ‰¹å¤§å° {batch_size} åœ¨ {self.device} ä¸Šè¿›è¡Œæ¨ç†")
        
        if str(self.device) == 'cuda':
            # æ˜¾ç¤ºGPUå†…å­˜ä¿¡æ¯
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated = torch.cuda.memory_allocated() / 1024**3
            print(f"ğŸ’¾ GPUå†…å­˜: {allocated:.1f}/{total_memory:.1f} GB")
        
        # æ‰¹é‡æ¨ç†
        start_time = time.time()
        results = self.predict_batch(image_paths, batch_size)
        total_time = time.time() - start_time
        
        # ç»Ÿè®¡
        success_count = sum(1 for r in results if r['success'])
        print(f"âœ… æˆåŠŸå¤„ç†: {success_count}/{len(image_paths)} å¼ å›¾ç‰‡")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’ ({len(image_paths)/total_time:.1f} å¼ /ç§’)")
        
        if str(self.device) == 'cuda':
            # æ˜¾ç¤ºæœ€ç»ˆGPUå†…å­˜ä½¿ç”¨
            allocated = torch.cuda.memory_allocated() / 1024**3
            print(f"ğŸ’¾ æœ€ç»ˆGPUå†…å­˜ä½¿ç”¨: {allocated:.1f} GB")
        
        return results
    
    def _preprocess_with_params(self, image: Union[str, Path, np.ndarray]) -> tuple:
        """å›¾åƒé¢„å¤„ç† - è¿”å›tensorå’Œå˜æ¢å‚æ•°ï¼ˆç”¨äºæ‰¹å¤„ç†ï¼‰"""
        # è¯»å–å›¾åƒ
        if isinstance(image, (str, Path)):
            image = cv2.imread(str(image))
            if image is None:
                raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image}")
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        elif len(image.shape) == 2:  # ç°åº¦å›¾
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        
        # ä¿å­˜åŸå§‹å°ºå¯¸
        original_size = image.shape[:2]  # (H, W)
        
        # åº”ç”¨Letterboxå˜æ¢
        image_letterboxed, transform_params = self.letterbox.apply(image)
        
        # ç”Ÿæˆvalid mask
        valid_mask = self.letterbox.create_padding_mask(transform_params)
        
        # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
        image_tensor = torch.from_numpy(image_letterboxed).float().permute(2, 0, 1) / 255.0
        valid_mask_tensor = torch.from_numpy(valid_mask).float().unsqueeze(0)
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray_tensor = 0.299 * image_tensor[0] + 0.587 * image_tensor[1] + 0.114 * image_tensor[2]
        gray_tensor = gray_tensor.unsqueeze(0)
        
        # ç»„åˆè¾“å…¥
        image_tensor = torch.cat([gray_tensor, valid_mask_tensor], dim=0)
        
        # æ·»åŠ batchç»´åº¦å¹¶ç§»åˆ°è®¾å¤‡
        image_tensor = image_tensor.unsqueeze(0).to(self.device)
        
        return image_tensor, transform_params, original_size
    
    def _preprocess(self, image: Union[str, Path, np.ndarray]) -> torch.Tensor:
        """å›¾åƒé¢„å¤„ç† - ä½¿ç”¨æ­£ç¡®çš„Letterboxå˜æ¢"""
        # è¯»å–å›¾åƒ
        if isinstance(image, (str, Path)):
            image = cv2.imread(str(image))
            if image is None:
                raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image}")
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        elif len(image.shape) == 2:  # ç°åº¦å›¾
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        
        # ä¿å­˜åŸå§‹å°ºå¯¸ç”¨äºåå¤„ç†
        self.original_size = image.shape[:2]  # (H, W)
        
        # åº”ç”¨Letterboxå˜æ¢ - ç­‰æ¯”ç¼©æ”¾ + å±…ä¸­å¡«å……
        image_letterboxed, self.transform_params = self.letterbox.apply(image)
        
        # ç”Ÿæˆvalid mask (validåŒºåŸŸ=1, paddingåŒºåŸŸ=0)
        valid_mask = self.letterbox.create_padding_mask(self.transform_params)
        
        # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ– [0, 255] -> [0, 1]
        image_tensor = torch.from_numpy(image_letterboxed).float().permute(2, 0, 1) / 255.0
        valid_mask_tensor = torch.from_numpy(valid_mask).float().unsqueeze(0)
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray_tensor = 0.299 * image_tensor[0] + 0.587 * image_tensor[1] + 0.114 * image_tensor[2]
        gray_tensor = gray_tensor.unsqueeze(0)  # æ·»åŠ é€šé“ç»´åº¦
        
        # ç»„åˆè¾“å…¥ [Grayscale(1) + Valid_Mask(1)]  
        # æœ€åä¸€ä¸ªé€šé“: 1=æœ‰æ•ˆåŒºåŸŸ, 0=paddingåŒºåŸŸ
        image_tensor = torch.cat([gray_tensor, valid_mask_tensor], dim=0)
        
        # æ·»åŠ batchç»´åº¦ [1, 2, 256, 512]
        image_tensor = image_tensor.unsqueeze(0).to(self.device)
        
        return image_tensor
    
    def _format_batch_result(self, predictions: Dict, batch_idx: int, 
                            transform_params: Dict, original_size: tuple) -> Dict:
        """æ ¼å¼åŒ–æ‰¹é‡æ¨ç†çš„å•ä¸ªç»“æœ"""
        # æå–æŒ‡å®šæ‰¹æ¬¡ç´¢å¼•çš„ç»“æœ
        gap_x_net = predictions['gap_coords'][batch_idx, 0].item()
        gap_y_net = predictions['gap_coords'][batch_idx, 1].item()
        slider_x_net = predictions['slider_coords'][batch_idx, 0].item()
        slider_y_net = predictions['slider_coords'][batch_idx, 1].item()
        
        # æ˜ å°„å›åŸå§‹å›¾åƒç©ºé—´
        gap_x, gap_y = self.coord_transform.input_to_original(
            (gap_x_net, gap_y_net), transform_params
        )
        slider_x, slider_y = self.coord_transform.input_to_original(
            (slider_x_net, slider_y_net), transform_params
        )
        
        # æå–ç½®ä¿¡åº¦
        gap_conf = predictions['gap_score'][batch_idx].item()
        slider_conf = predictions['slider_score'][batch_idx].item()
        
        return {
            'sliding_distance': gap_x - slider_x,
            'gap_x': gap_x,
            'gap_y': gap_y,
            'slider_x': slider_x,
            'slider_y': slider_y,
            'gap_confidence': gap_conf,
            'slider_confidence': slider_conf,
            'confidence': (gap_conf + slider_conf) / 2,
            'details': {
                'gap_coords': [gap_x, gap_y],
                'slider_coords': [slider_x, slider_y],
                'model_version': '1.1.0',
                'device_used': str(self.device),
                'original_size': original_size,
                'network_coords': {
                    'gap': [gap_x_net, gap_y_net],
                    'slider': [slider_x_net, slider_y_net]
                }
            }
        }
    
    def _format_result(self, predictions: Dict) -> Dict:
        """æ ¼å¼åŒ–è¾“å‡ºç»“æœ - ç›´æ¥ä½¿ç”¨æ¨¡å‹è§£ç çš„åæ ‡"""
        # æ³¨æ„ï¼šæ¨¡å‹å·²ç»åœ¨decode_predictionsä¸­å®Œæˆäº†æ‰€æœ‰å¤„ç†
        # åŒ…æ‹¬padding maskå±è”½å’Œåæ ‡clampåˆ°[0,512]x[0,256]
        # è¿™é‡Œåªéœ€è¦æå–å¹¶æ˜ å°„å›åŸå§‹å›¾åƒç©ºé—´
        
        # æå–ç½‘ç»œè¾“å‡ºçš„åæ ‡ï¼ˆåœ¨512x256ç©ºé—´ä¸­ï¼‰
        gap_x_net = predictions['gap_coords'][0, 0].item()
        gap_y_net = predictions['gap_coords'][0, 1].item()
        slider_x_net = predictions['slider_coords'][0, 0].item()
        slider_y_net = predictions['slider_coords'][0, 1].item()
        
        # å°†åæ ‡ä»ç½‘ç»œç©ºé—´æ˜ å°„å›åŸå§‹å›¾åƒç©ºé—´
        gap_x, gap_y = self.coord_transform.input_to_original(
            (gap_x_net, gap_y_net), self.transform_params
        )
        slider_x, slider_y = self.coord_transform.input_to_original(
            (slider_x_net, slider_y_net), self.transform_params
        )
        
        # æå–ç½®ä¿¡åº¦
        gap_conf = predictions['gap_score'][0].item()
        slider_conf = predictions['slider_score'][0].item()
        
        return {
            'sliding_distance': gap_x - slider_x,
            'gap_x': gap_x,
            'gap_y': gap_y,
            'slider_x': slider_x,
            'slider_y': slider_y,
            'gap_confidence': gap_conf,
            'slider_confidence': slider_conf,
            'confidence': (gap_conf + slider_conf) / 2,
            'details': {
                'gap_coords': [gap_x, gap_y],
                'slider_coords': [slider_x, slider_y],
                'model_version': '1.1.0',
                'device_used': str(self.device),
                'original_size': self.original_size,
                'network_coords': {
                    'gap': [gap_x_net, gap_y_net],
                    'slider': [slider_x_net, slider_y_net]
                }
            }
        }
    
    def visualize_prediction(self, 
                           image_path: Union[str, Path],
                           save_path: Optional[str] = None,
                           show: bool = True) -> None:
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ
        
        Args:
            image_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            show: æ˜¯å¦æ˜¾ç¤ºå›¾ç‰‡
        """
        import matplotlib.pyplot as plt
        import matplotlib.patches as patches
        
        # è·å–é¢„æµ‹ç»“æœ
        result = self.predict(image_path)
        if not result['success']:
            print(f"é¢„æµ‹å¤±è´¥: {result.get('error', 'Unknown error')}")
            return
        
        # è¯»å–åŸå›¾
        if isinstance(image_path, (str, Path)):
            image = cv2.imread(str(image_path))
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image = image_path
        
        # åˆ›å»ºå›¾å½¢
        fig, ax = plt.subplots(1, 1, figsize=(10, 5))
        ax.imshow(image)
        
        # ç»˜åˆ¶ç¼ºå£ä½ç½®ï¼ˆçº¢è‰²ï¼‰
        gap_rect = patches.Circle((result['gap_x'], result['gap_y']), 
                                 20, linewidth=2, edgecolor='red', 
                                 facecolor='none', label='Gap')
        ax.add_patch(gap_rect)
        
        # ç»˜åˆ¶æ»‘å—ä½ç½®ï¼ˆç»¿è‰²ï¼‰
        slider_rect = patches.Circle((result['slider_x'], result['slider_y']), 
                                    20, linewidth=2, edgecolor='green', 
                                    facecolor='none', label='Slider')
        ax.add_patch(slider_rect)
        
        # ç»˜åˆ¶æ»‘åŠ¨è·ç¦»çº¿ï¼ˆè“è‰²ï¼‰
        ax.arrow(result['slider_x'], result['slider_y'],
                result['sliding_distance'], 0,
                head_width=10, head_length=10, fc='blue', ec='blue',
                alpha=0.7, label=f"Distance: {result['sliding_distance']:.1f}px")
        
        # æ·»åŠ æ ‡é¢˜å’Œå›¾ä¾‹
        ax.set_title(f"æ»‘åŠ¨è·ç¦»: {result['sliding_distance']:.1f}px | "
                    f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
        ax.legend()
        ax.axis('off')
        
        # ä¿å­˜æˆ–æ˜¾ç¤º
        if save_path:
            plt.savefig(save_path, dpi=100, bbox_inches='tight')
            print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        
        if show:
            plt.show()
        else:
            plt.close()
    
    def visualize_heatmaps(self,
                         image_path: Union[str, Path],
                         save_path: Optional[str] = None,
                         show: bool = True) -> None:
        """å¯è§†åŒ–çƒ­åŠ›å›¾
        
        Args:
            image_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            show: æ˜¯å¦æ˜¾ç¤ºå›¾ç‰‡
        """
        import matplotlib.pyplot as plt
        
        # é¢„å¤„ç†å›¾åƒ
        image_tensor = self._preprocess(image_path)
        
        # è·å–çƒ­åŠ›å›¾
        with torch.no_grad():
            outputs = self.model(image_tensor)
        
        # è¯»å–åŸå›¾
        if isinstance(image_path, (str, Path)):
            image = cv2.imread(str(image_path))
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image = image_path
        
        # æå–çƒ­åŠ›å›¾
        gap_heatmap = outputs['heatmap_gap'][0, 0].cpu().numpy()
        slider_heatmap = outputs['heatmap_slider'][0, 0].cpu().numpy()
        
        # ä¸Šé‡‡æ ·çƒ­åŠ›å›¾åˆ°åŸå›¾å°ºå¯¸
        h, w = image.shape[:2]
        gap_heatmap = cv2.resize(gap_heatmap, (w, h))
        slider_heatmap = cv2.resize(slider_heatmap, (w, h))
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # åŸå›¾
        axes[0].imshow(image)
        axes[0].set_title("åŸå›¾")
        axes[0].axis('off')
        
        # ç¼ºå£çƒ­åŠ›å›¾
        im1 = axes[1].imshow(gap_heatmap, cmap='hot', alpha=0.8)
        axes[1].imshow(image, alpha=0.3)
        axes[1].set_title("ç¼ºå£çƒ­åŠ›å›¾")
        axes[1].axis('off')
        plt.colorbar(im1, ax=axes[1], fraction=0.046)
        
        # æ»‘å—çƒ­åŠ›å›¾
        im2 = axes[2].imshow(slider_heatmap, cmap='hot', alpha=0.8)
        axes[2].imshow(image, alpha=0.3)
        axes[2].set_title("æ»‘å—çƒ­åŠ›å›¾")
        axes[2].axis('off')
        plt.colorbar(im2, ax=axes[2], fraction=0.046)
        
        plt.tight_layout()
        
        # ä¿å­˜æˆ–æ˜¾ç¤º
        if save_path:
            plt.savefig(save_path, dpi=100, bbox_inches='tight')
            print(f"âœ… çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")
        
        if show:
            plt.show()
        else:
            plt.close()